{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BYOL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyuhyoJeon/BYOL/blob/master/BYOL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U0mliCb7ZTj"
      },
      "source": [
        "### Google drive mount ###\r\n",
        "# from google.colab import drive \r\n",
        "# drive.mount('/content/gdrive/')\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbTSofU9NHbz"
      },
      "source": [
        "### Arguments define ###\r\n",
        "import easydict\r\n",
        "import os\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "args = easydict.EasyDict({\r\n",
        "    'image_size':32, # original = 224\r\n",
        "    'learning_rate':0.2, # original lr = 0.2, others = 0.3 or 3e-4\r\n",
        "    'momentum':0, \r\n",
        "    'weight_decay':1.5e-6, \r\n",
        "    'batch_size':1024, \r\n",
        "    'num_epochs':1000, \r\n",
        "    'resnet_version':'resnet18', # original = resnet50\r\n",
        "    'optim':'lars', # 'lars', 'adam', 'sgd' \r\n",
        "    'checkpoint_epochs':10, \r\n",
        "    # ********************MUST CHECK********************** #\r\n",
        "    'dataset_dir':'/content/gdrive/MyDrive/Colab Notebooks/datasets', # dataset directory\r\n",
        "    'ckpt_dir':'/content/gdrive/MyDrive/Colab Notebooks/byol/ckpt',   # Network checkpoint directory\r\n",
        "    'num_workers':8, \r\n",
        "    'nodes':1, \r\n",
        "    'gpus':1, \r\n",
        "    'nr':0, \r\n",
        "    'device':'cuda', \r\n",
        "    'eval':True, \r\n",
        "    'eval_epochs':30, \r\n",
        "    # ********************MUST CHECK********************** #\r\n",
        "    'dryrun':True, # check line 47~53\r\n",
        "    'debug':True, # check line 56~62\r\n",
        "    'current_epochs':0\r\n",
        "})\r\n",
        "\r\n",
        "# ********************MUST CHECK********************** #\r\n",
        "# dryrun setting\r\n",
        "if args.dryrun:\r\n",
        "  args.image_size=32\r\n",
        "  args.num_epochs = 100\r\n",
        "  args.batch_size = 256\r\n",
        "  args.num_workers = 4\r\n",
        "  args.dryrun_subset_size = 100\r\n",
        "  args.resnet_version = 'resnet18'\r\n",
        "\r\n",
        "# ********************MUST CHECK********************** #\r\n",
        "# debug setting\r\n",
        "if args.debug:\r\n",
        "  args.image_size=32\r\n",
        "  args.num_epochs = 1\r\n",
        "  args.batch_size = 2\r\n",
        "  args.num_workers = 0\r\n",
        "  args.debug_subset_size = 8\r\n",
        "  args.resnet_version = 'resnet18'\r\n",
        "  \r\n",
        "\r\n",
        "# make check point directory ex: \"ckpt_dir/resnet18/lars/021805\"\r\n",
        "tmp_dir = os.path.join(args.ckpt_dir, f\"{args.resnet_version}\", f\"{args.optim}\", f\"{datetime.now().strftime('%m%d%H')}\")\r\n",
        "if not os.path.exists(tmp_dir):\r\n",
        "  os.makedirs(tmp_dir)\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwAPPgDwNo2j"
      },
      "source": [
        "### Image augmentation define\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "from torchvision import datasets, transforms\r\n",
        "\r\n",
        "class simclr_transform:\r\n",
        "  # augmentations: \r\n",
        "  # random patch, 224 resize, random hrizontal flip, color distortion, \r\n",
        "  # random swquence brightness, contrast, saturation, hue adjustment, \r\n",
        "  # and optional gray scale conversion, Gaussian blur, solarization\r\n",
        "  imagenet_mean_std = [[0.485, 0.456, 0.406],[0.229, 0.224, 0.225]]\r\n",
        "  def __init__(self, size, mean_std=imagenet_mean_std, s=1.0):\r\n",
        "    self.transform = transforms.Compose(\r\n",
        "        [\r\n",
        "        transforms.RandomSizedCrop(size=size), \r\n",
        "        transforms.RandomHorizontalFlip(), \r\n",
        "        transforms.RandomApply(\r\n",
        "            [transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)], p=0.8),\r\n",
        "        transforms.RandomGrayscale(p=0.2),\r\n",
        "        transforms.RandomApply(\r\n",
        "            [transforms.GaussianBlur(kernel_size=size//20*2+1, sigma=(0.1, 2.0))], p=0.5), \r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize(*mean_std)\r\n",
        "        ]\r\n",
        "    )\r\n",
        "  def __call__(self, x):\r\n",
        "    x1 = self.transform(x)\r\n",
        "    x2 = self.transform(x)\r\n",
        "    return x1, x2\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "class Transform_single():\r\n",
        "  imagenet_mean_std = [[0.485, 0.456, 0.406],[0.229, 0.224, 0.225]]\r\n",
        "  def __init__(self, size, train, normalize=imagenet_mean_std):\r\n",
        "    if train == True:\r\n",
        "      self.transform = transforms.Compose(\r\n",
        "          [\r\n",
        "           transforms.RandomResizedCrop(size, scale=(0.08, 1.0), \r\n",
        "                                        ratio=(3.0/4.0,4.0/3.0), \r\n",
        "                                        interpolation=Image.BICUBIC\r\n",
        "                                        ),\r\n",
        "           transforms.RandomHorizontalFlip(),\r\n",
        "           transforms.ToTensor(),\r\n",
        "           transforms.Normalize(*normalize)\r\n",
        "          ]\r\n",
        "      )\r\n",
        "    else:\r\n",
        "      self.transform = transforms.Compose(\r\n",
        "          [\r\n",
        "           transforms.Resize(int(size*(8/7)), \r\n",
        "                             interpolation=Image.BICUBIC\r\n",
        "                             ), # 224 -> 256 \r\n",
        "           transforms.CenterCrop(size),\r\n",
        "           transforms.ToTensor(),\r\n",
        "           transforms.Normalize(*normalize)\r\n",
        "          ]\r\n",
        "      )\r\n",
        "\r\n",
        "  def __call__(self, x):\r\n",
        "    return self.transform(x)\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5jUb9pev062",
        "outputId": "91363cbf-6aac-4b0f-bdf2-f42f65c069ae"
      },
      "source": [
        "### dataset load ###\r\n",
        "cifar_train = datasets.CIFAR10(\r\n",
        "    root=args.dataset_dir, \r\n",
        "    train=True, \r\n",
        "    transform=simclr_transform(args.image_size), \r\n",
        "    download=True\r\n",
        ")\r\n",
        "\r\n",
        "cifar_memory = datasets.CIFAR10(\r\n",
        "    root=args.dataset_dir, \r\n",
        "    train=True, \r\n",
        "    download=False, \r\n",
        "    transform=Transform_single(size=args.image_size, train=False), \r\n",
        "    )\r\n",
        "\r\n",
        "cifar_test = datasets.CIFAR10(\r\n",
        "    root=args.dataset_dir, \r\n",
        "    train=False, \r\n",
        "    download=False, \r\n",
        "    transform=Transform_single(size=args.image_size, train=False), \r\n",
        ")\r\n",
        "\r\n",
        "if args.debug:\r\n",
        "  cifar_train = torch.utils.data.Subset(cifar_train, range(0, args.debug_subset_size))\r\n",
        "  cifar_train.classes = cifar_train.dataset.classes\r\n",
        "  cifar_train.targets = cifar_train.dataset.targets\r\n",
        "  cifar_memory = torch.utils.data.Subset(cifar_memory, range(0, args.debug_subset_size))\r\n",
        "  cifar_memory.classes = cifar_memory.dataset.classes\r\n",
        "  cifar_memory.targets = cifar_memory.dataset.targets\r\n",
        "  cifar_test = torch.utils.data.Subset(cifar_test, range(0, args.debug_subset_size))\r\n",
        "  cifar_test.classes = cifar_test.dataset.classes\r\n",
        "  cifar_test.targets = cifar_test.dataset.targets\r\n",
        "# elif args.dryrun:\r\n",
        "#   cifar_train = torch.utils.data.Subset(cifar_train, range(0, args.dryrun_subset_size))\r\n",
        "#   cifar_train.classes = cifar_train.dataset.classes\r\n",
        "#   cifar_train.targets = cifar_train.dataset.targets\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(\r\n",
        "    cifar_train, \r\n",
        "    batch_size=args.batch_size, \r\n",
        "    shuffle=True, \r\n",
        "    num_workers=args.num_workers, \r\n",
        "    drop_last=True, \r\n",
        "    pin_memory=True\r\n",
        ")\r\n",
        "\r\n",
        "memory_loader = torch.utils.data.DataLoader(\r\n",
        "    cifar_memory, \r\n",
        "    shuffle=False,\r\n",
        "    batch_size=args.batch_size,\r\n",
        "    num_workers=args.num_workers,\r\n",
        "    drop_last=True,\r\n",
        "    pin_memory=True,\r\n",
        "    )\r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(\r\n",
        "    cifar_test, \r\n",
        "    shuffle=False,\r\n",
        "    batch_size=args.batch_size,\r\n",
        "    num_workers=args.num_workers,\r\n",
        "    drop_last=True,\r\n",
        "    pin_memory=True,\r\n",
        ")\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:841: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
            "  \"please use transforms.RandomResizedCrop instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sSKUET0IL1c"
      },
      "source": [
        "### dataset load check ###\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# def imshow(img):\r\n",
        "#   img = img / 2 + 0.5     # unnormalize\r\n",
        "#   npimg = img.numpy()\r\n",
        "#   plt.imshow(np.transpose(npimg, (1, 2, 0)))\r\n",
        "#   plt.show()\r\n",
        "\r\n",
        "# dataiter = iter(train_loader)\r\n",
        "# (images1, images2), labels = dataiter.next()\r\n",
        "\r\n",
        "# imshow(torchvision.utils.make_grid(images1))\r\n",
        "# imshow(torchvision.utils.make_grid(images2))\r\n",
        "# print(' '.join('%5s' % train_loader.dataset.classes[labels[j]] for j in range(len(labels))))\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2E6ASOUo7PV"
      },
      "source": [
        "### ResNet18 for CIFAR10 define ###\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import os\r\n",
        "# https://raw.githubusercontent.com/huyvnphan/PyTorch_CIFAR10/master/cifar10_models/resnet.py\r\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\r\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d']\r\n",
        "\r\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\r\n",
        "  \"\"\"3x3 convolution with padding\"\"\"\r\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n",
        "                    padding=dilation, groups=groups, bias=False, dilation=dilation)\r\n",
        "\r\n",
        "\r\n",
        "def conv1x1(in_planes, out_planes, stride=1):\r\n",
        "  \"\"\"1x1 convolution\"\"\"\r\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\r\n",
        "\r\n",
        "\r\n",
        "class BasicBlock(nn.Module):\r\n",
        "  expansion = 1\r\n",
        "\r\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\r\n",
        "                base_width=64, dilation=1, norm_layer=None):\r\n",
        "    super(BasicBlock, self).__init__()\r\n",
        "    if norm_layer is None:\r\n",
        "      norm_layer = nn.BatchNorm2d\r\n",
        "    if groups != 1 or base_width != 64:\r\n",
        "      raise ValueError('BasicBlock only supports groups=1 and base_width=64')\r\n",
        "    if dilation > 1:\r\n",
        "      raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\r\n",
        "    # Both self.conv1 and self.downsample layers downsample the input when stride != 1\r\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\r\n",
        "    self.bn1 = norm_layer(planes)\r\n",
        "    self.relu = nn.ReLU(inplace=True)\r\n",
        "    self.conv2 = conv3x3(planes, planes)\r\n",
        "    self.bn2 = norm_layer(planes)\r\n",
        "    self.downsample = downsample\r\n",
        "    self.stride = stride\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    identity = x\r\n",
        "\r\n",
        "    out = self.conv1(x)\r\n",
        "    out = self.bn1(out)\r\n",
        "    out = self.relu(out)\r\n",
        "\r\n",
        "    out = self.conv2(out)\r\n",
        "    out = self.bn2(out)\r\n",
        "\r\n",
        "    if self.downsample is not None:\r\n",
        "        identity = self.downsample(x)\r\n",
        "\r\n",
        "    out += identity\r\n",
        "    out = self.relu(out)\r\n",
        "\r\n",
        "    return out\r\n",
        "\r\n",
        "\r\n",
        "class Bottleneck(nn.Module):\r\n",
        "  expansion = 4\r\n",
        "\r\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\r\n",
        "                base_width=64, dilation=1, norm_layer=None):\r\n",
        "    super(Bottleneck, self).__init__()\r\n",
        "    if norm_layer is None:\r\n",
        "        norm_layer = nn.BatchNorm2d\r\n",
        "    width = int(planes * (base_width / 64.)) * groups\r\n",
        "    # Both self.conv2 and self.downsample layers downsample the input when stride != 1\r\n",
        "    self.conv1 = conv1x1(inplanes, width)\r\n",
        "    self.bn1 = norm_layer(width)\r\n",
        "    self.conv2 = conv3x3(width, width, stride, groups, dilation)\r\n",
        "    self.bn2 = norm_layer(width)\r\n",
        "    self.conv3 = conv1x1(width, planes * self.expansion)\r\n",
        "    self.bn3 = norm_layer(planes * self.expansion)\r\n",
        "    self.relu = nn.ReLU(inplace=True)\r\n",
        "    self.downsample = downsample\r\n",
        "    self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "      identity = x\r\n",
        "\r\n",
        "      out = self.conv1(x)\r\n",
        "      out = self.bn1(out)\r\n",
        "      out = self.relu(out)\r\n",
        "\r\n",
        "      out = self.conv2(out)\r\n",
        "      out = self.bn2(out)\r\n",
        "      out = self.relu(out)\r\n",
        "\r\n",
        "      out = self.conv3(out)\r\n",
        "      out = self.bn3(out)\r\n",
        "\r\n",
        "      if self.downsample is not None:\r\n",
        "          identity = self.downsample(x)\r\n",
        "\r\n",
        "      out += identity\r\n",
        "      out = self.relu(out)\r\n",
        "\r\n",
        "      return out\r\n",
        "\r\n",
        "\r\n",
        "class ResNet(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self, block, layers, num_classes=10, zero_init_residual=False,\r\n",
        "                groups=1, width_per_group=64, replace_stride_with_dilation=None,\r\n",
        "                norm_layer=None):\r\n",
        "    super(ResNet, self).__init__()\r\n",
        "    if norm_layer is None:\r\n",
        "      norm_layer = nn.BatchNorm2d\r\n",
        "    self._norm_layer = norm_layer\r\n",
        "\r\n",
        "    self.inplanes = 64\r\n",
        "    self.dilation = 1\r\n",
        "    if replace_stride_with_dilation is None:\r\n",
        "      # each element in the tuple indicates if we should replace\r\n",
        "      # the 2x2 stride with a dilated convolution instead\r\n",
        "      replace_stride_with_dilation = [False, False, False]\r\n",
        "    if len(replace_stride_with_dilation) != 3:\r\n",
        "      raise ValueError(\"replace_stride_with_dilation should be None \"\r\n",
        "                        \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\r\n",
        "    self.groups = groups\r\n",
        "    self.base_width = width_per_group\r\n",
        "    \r\n",
        "    ## CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\r\n",
        "    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\r\n",
        "    ## END\r\n",
        "    \r\n",
        "    self.bn1 = norm_layer(self.inplanes)\r\n",
        "    self.relu = nn.ReLU(inplace=True)\r\n",
        "    # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
        "    self.layer1 = self._make_layer(block, 64, layers[0])\r\n",
        "    self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\r\n",
        "                                    dilate=replace_stride_with_dilation[0])\r\n",
        "    self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\r\n",
        "                                    dilate=replace_stride_with_dilation[1])\r\n",
        "    self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\r\n",
        "                                    dilate=replace_stride_with_dilation[2])\r\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n",
        "    self.fc = nn.Linear(512 * block.expansion, num_classes)\r\n",
        "\r\n",
        "    for m in self.modules():\r\n",
        "      if isinstance(m, nn.Conv2d):\r\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n",
        "      elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\r\n",
        "        nn.init.constant_(m.weight, 1)\r\n",
        "        nn.init.constant_(m.bias, 0)\r\n",
        "\r\n",
        "    # Zero-initialize the last BN in each residual branch,\r\n",
        "    # so that the residual branch starts with zeros, and each residual block behaves like an identity.\r\n",
        "    # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\r\n",
        "    if zero_init_residual:\r\n",
        "      for m in self.modules():\r\n",
        "        if isinstance(m, Bottleneck):\r\n",
        "          nn.init.constant_(m.bn3.weight, 0)\r\n",
        "        elif isinstance(m, BasicBlock):\r\n",
        "          nn.init.constant_(m.bn2.weight, 0)\r\n",
        "\r\n",
        "  def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\r\n",
        "    norm_layer = self._norm_layer\r\n",
        "    downsample = None\r\n",
        "    previous_dilation = self.dilation\r\n",
        "    if dilate:\r\n",
        "        self.dilation *= stride\r\n",
        "        stride = 1\r\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\r\n",
        "        downsample = nn.Sequential(\r\n",
        "            conv1x1(self.inplanes, planes * block.expansion, stride),\r\n",
        "            norm_layer(planes * block.expansion),\r\n",
        "        )\r\n",
        "\r\n",
        "    layers = []\r\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\r\n",
        "                        self.base_width, previous_dilation, norm_layer))\r\n",
        "    self.inplanes = planes * block.expansion\r\n",
        "    for _ in range(1, blocks):\r\n",
        "        layers.append(block(self.inplanes, planes, groups=self.groups,\r\n",
        "                            base_width=self.base_width, dilation=self.dilation,\r\n",
        "                            norm_layer=norm_layer))\r\n",
        "\r\n",
        "    return nn.Sequential(*layers)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.bn1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    # x = self.maxpool(x)\r\n",
        "\r\n",
        "    x = self.layer1(x)\r\n",
        "    x = self.layer2(x)\r\n",
        "    x = self.layer3(x)\r\n",
        "    x = self.layer4(x)\r\n",
        "\r\n",
        "    x = self.avgpool(x)\r\n",
        "    x = x.reshape(x.size(0), -1)\r\n",
        "    x = self.fc(x)\r\n",
        "\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def _resnet(arch, block, layers, pretrained, progress, device, **kwargs):\r\n",
        "  model = ResNet(block, layers, **kwargs)\r\n",
        "  if pretrained:\r\n",
        "      script_dir = os.path.dirname(__file__)\r\n",
        "      state_dict = torch.load(script_dir + '/state_dicts/'+arch+'.pt', map_location=device)\r\n",
        "      model.load_state_dict(state_dict)\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "def resnet18(pretrained=False, progress=True, device='cpu', **kwargs):\r\n",
        "  \"\"\"Constructs a ResNet-18 model.\r\n",
        "  Args:\r\n",
        "      pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "      progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "  \"\"\"\r\n",
        "  return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress, device,\r\n",
        "                  **kwargs)\r\n",
        "\r\n",
        "\r\n",
        "def resnet34(pretrained=False, progress=True, device='cpu', **kwargs):\r\n",
        "  \"\"\"Constructs a ResNet-34 model.\r\n",
        "  Args:\r\n",
        "      pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "      progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "  \"\"\"\r\n",
        "  return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress, device,\r\n",
        "                  **kwargs)\r\n",
        "\r\n",
        "\r\n",
        "def resnet50(pretrained=False, progress=True, device='cpu', **kwargs):\r\n",
        "  \"\"\"Constructs a ResNet-50 model.\r\n",
        "  Args:\r\n",
        "      pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "      progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "  \"\"\"\r\n",
        "  return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, device,\r\n",
        "                  **kwargs)\r\n",
        "\r\n",
        "\r\n",
        "def resnet101(pretrained=False, progress=True, device='cpu', **kwargs):\r\n",
        "  \"\"\"Constructs a ResNet-101 model.\r\n",
        "  Args:\r\n",
        "      pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "      progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "  \"\"\"\r\n",
        "  return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress, device,\r\n",
        "                  **kwargs)\r\n",
        "\r\n",
        "\r\n",
        "def resnet152(pretrained=False, progress=True, device='cpu', **kwargs):\r\n",
        "  \"\"\"Constructs a ResNet-152 model.\r\n",
        "  Args:\r\n",
        "      pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "      progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "  \"\"\"\r\n",
        "  return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress, device,\r\n",
        "                  **kwargs)\r\n",
        "\r\n",
        "\r\n",
        "def resnext50_32x4d(pretrained=False, progress=True, device='cpu', **kwargs):\r\n",
        "  \"\"\"Constructs a ResNeXt-50 32x4d model.\r\n",
        "  Args:\r\n",
        "      pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "      progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "  \"\"\"\r\n",
        "  kwargs['groups'] = 32\r\n",
        "  kwargs['width_per_group'] = 4\r\n",
        "  return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\r\n",
        "                  pretrained, progress, device, **kwargs)\r\n",
        "\r\n",
        "\r\n",
        "def resnext101_32x8d(pretrained=False, progress=True, device='cpu', **kwargs):\r\n",
        "  \"\"\"Constructs a ResNeXt-101 32x8d model.\r\n",
        "  Args:\r\n",
        "      pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "      progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "  \"\"\"\r\n",
        "  kwargs['groups'] = 32\r\n",
        "  kwargs['width_per_group'] = 8\r\n",
        "  return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\r\n",
        "                  pretrained, progress, device, **kwargs)\r\n",
        "\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh_OjbkTKGV-"
      },
      "source": [
        "### resnet call ###\r\n",
        "import torch.nn as nn\r\n",
        "from torchvision import models\r\n",
        "\r\n",
        "if args.resnet_version is not None:\r\n",
        "  resnet = eval(f'{args.resnet_version}()')\r\n",
        "  # resnet = eval(f'models.{args.resnet_version}()') # Original torchvision models\r\n",
        "  resnet.output_dim = resnet.fc.in_features\r\n",
        "  resnet.fc = nn.Identity()\r\n",
        "else:\r\n",
        "  raise NotImplementedError(\"Backbone is not implemented!\")\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDkTLBPcy1lv"
      },
      "source": [
        "### byol network define ###\r\n",
        "import copy\r\n",
        "import math\r\n",
        "from torch.nn import functional\r\n",
        "\r\n",
        "hidden_size=4096\r\n",
        "projection_size=256\r\n",
        "\r\n",
        "class MLP(nn.Module):\r\n",
        "  def __init__(self, input_dim):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.net = nn.Sequential(\r\n",
        "        nn.Linear(input_dim, hidden_size), \r\n",
        "        nn.BatchNorm1d(hidden_size, momentum=1-0.9, eps=1e-5), \r\n",
        "        nn.ReLU(inplace=True), \r\n",
        "        nn.Linear(hidden_size, projection_size)\r\n",
        "    )\r\n",
        "  def forward(self, x):\r\n",
        "    return self.net(x)\r\n",
        "\r\n",
        "class BYOL(nn.Module):\r\n",
        "  def __init__(self, backbone):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.backbone=backbone\r\n",
        "    self.projector = MLP(resnet.output_dim)\r\n",
        "    self.online_encoder = nn.Sequential(\r\n",
        "        self.backbone, \r\n",
        "        self.projector,\r\n",
        "    )\r\n",
        "    self.predictor = MLP(projection_size)\r\n",
        "    self.target_encoder = copy.deepcopy(self.online_encoder)\r\n",
        "\r\n",
        "  def target_ema(self, k, K, base_tau=0.996):\r\n",
        "    return 1-(1-base_tau)*(math.cos(math.pi*k/K)+1)/2\r\n",
        "\r\n",
        "  def reset_moving_average(self):\r\n",
        "    del self.target_encoder\r\n",
        "    self.target_encoder = copy.deepcopy(self.online_encoder)\r\n",
        "\r\n",
        "  def update_moving_average(self, global_step, max_steps):\r\n",
        "    tau = self.target_ema(global_step, max_steps)\r\n",
        "    for online, target in zip(self.online_encoder.parameters(), self.target_encoder.parameters()):\r\n",
        "      target.data = tau*target.data + (1-tau)*online.data\r\n",
        "  \r\n",
        "  def loss_function(self, p, z):\r\n",
        "    p=functional.normalize(p, dim=-1, p=2)\r\n",
        "    z=functional.normalize(z, dim=-1, p=2)\r\n",
        "    return 2 - 2*(p*z).sum(dim=-1)\r\n",
        "\r\n",
        "  def forward(self, x1, x2):\r\n",
        "    z1_online, z2_online = self.online_encoder(x1), self.online_encoder(x2)\r\n",
        "    p1_online, p2_online = self.predictor(z1_online), self.predictor(z2_online)\r\n",
        "    with torch.no_grad():\r\n",
        "      z1_target, z2_target = self.target_encoder(x1), self.target_encoder(x2)\r\n",
        "    \r\n",
        "    loss1, loss2 = self.loss_function(p1_online, z2_target.detach()), self.loss_function(p2_online, z1_target.detach())\r\n",
        "\r\n",
        "    loss = loss1+loss2\r\n",
        "    return loss.mean()\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoNT2XuVRa_t"
      },
      "source": [
        "### byol network call ###\r\n",
        "byol = BYOL(resnet)\r\n",
        "byol = byol.to(args.device)\r\n",
        "byol = torch.nn.DataParallel(byol)\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKix7CVrSenZ"
      },
      "source": [
        "### Lars optimizer define ###\r\n",
        "from torch.optim import Adam, SGD\r\n",
        "from torch.optim.optimizer import Optimizer\r\n",
        "\r\n",
        "class LARS(Optimizer):\r\n",
        "  def __init__(self, named_modules, lr, momentum=0.9, trust_coef=1e-3, weight_decay=1.5e-6, exclude_bias_from_adaption=True):\r\n",
        "    defaults = dict(momentum=momentum, lr=lr, weight_decay=weight_decay, trust_coef=trust_coef)\r\n",
        "    parameters = self.exclude_from_model(named_modules, exclude_bias_from_adaption)\r\n",
        "    super(LARS, self).__init__(parameters, defaults)\r\n",
        "\r\n",
        "  @torch.no_grad() \r\n",
        "  def step(self):\r\n",
        "    for group in self.param_groups: # only 1 group in most cases \r\n",
        "      weight_decay = group['weight_decay']\r\n",
        "      momentum = group['momentum']\r\n",
        "      lr = group['lr']\r\n",
        "      trust_coef = group['trust_coef']\r\n",
        "      # print(group['name'])\r\n",
        "      # eps = group['eps']\r\n",
        "      for p in group['params']:\r\n",
        "        # breakpoint()\r\n",
        "        if p.grad is None:\r\n",
        "          continue\r\n",
        "        global_lr = lr\r\n",
        "        velocity = self.state[p].get('velocity', 0)  \r\n",
        "        # if name in self.exclude_from_layer_adaptation:\r\n",
        "        if self._use_weight_decay(group):\r\n",
        "          p.grad.data += weight_decay * p.data \r\n",
        "\r\n",
        "        trust_ratio = 1.0 \r\n",
        "        if self._do_layer_adaptation(group):\r\n",
        "          w_norm = torch.norm(p.data, p=2)\r\n",
        "          g_norm = torch.norm(p.grad.data, p=2)\r\n",
        "          trust_ratio = trust_coef * w_norm / g_norm if w_norm > 0 and g_norm > 0 else 1.0 \r\n",
        "        scaled_lr = global_lr * trust_ratio # trust_ratio is the local_lr \r\n",
        "        next_v = momentum * velocity + scaled_lr * p.grad.data \r\n",
        "        update = next_v\r\n",
        "        p.data = p.data - update \r\n",
        "\r\n",
        "  def _use_weight_decay(self, group):\r\n",
        "    return False if group['name'] == 'exclude' else True\r\n",
        "  def _do_layer_adaptation(self, group):\r\n",
        "    return False if group['name'] == 'exclude' else True\r\n",
        "\r\n",
        "  def exclude_from_model(self, named_modules, exclude_bias_from_adaption=True):\r\n",
        "    base = [] \r\n",
        "    exclude = []\r\n",
        "    for name, module in named_modules:\r\n",
        "      if type(module) in [nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]:\r\n",
        "        # if isinstance(module, torch.nn.modules.batchnorm._BatchNorm)\r\n",
        "        for name2, param in module.named_parameters():\r\n",
        "          exclude.append(param)\r\n",
        "      else:\r\n",
        "        for name2, param in module.named_parameters():\r\n",
        "          if name2 == 'bias':\r\n",
        "            exclude.append(param)\r\n",
        "          elif name2 == 'weight':\r\n",
        "            base.append(param)\r\n",
        "          else:\r\n",
        "            pass # non leaf modules \r\n",
        "    return [{\r\n",
        "        'name': 'base',\r\n",
        "        'params': base\r\n",
        "        },{\r\n",
        "        'name': 'exclude',\r\n",
        "        'params': exclude\r\n",
        "    }] if exclude_bias_from_adaption == True else [{\r\n",
        "        'name': 'base',\r\n",
        "        'params': base+exclude \r\n",
        "    }]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn6oc9ztTiRR"
      },
      "source": [
        "LARS optimizer is from Github PatrickHua/SimSiam\r\n",
        "\r\n",
        "\r\n",
        "> Link: https://github.com/PatrickHua/SimSiam/blob/main/optimizers/lars_simclr.py\r\n",
        "\r\n",
        "\r\n",
        "### ------------------------------------------ ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VonOn4aaUCV2"
      },
      "source": [
        "### optimizer call ###\r\n",
        "predictor_prefix = ('module.predictor', 'predictor')\r\n",
        "parameters = [{\r\n",
        "    'name': 'base',\r\n",
        "    'params': [param for name, param in byol.named_parameters() if not name.startswith(predictor_prefix)],\r\n",
        "    'lr': args.learning_rate\r\n",
        "},{\r\n",
        "    'name': 'predictor',\r\n",
        "    'params': [param for name, param in byol.named_parameters() if name.startswith(predictor_prefix)],\r\n",
        "    'lr': args.learning_rate\r\n",
        "}]\r\n",
        "if args.optim == 'lars':\r\n",
        "  optimizer = LARS(byol.named_modules(), lr=args.learning_rate*args.batch_size/256, weight_decay=args.weight_decay)\r\n",
        "elif args.optim == 'adam':\r\n",
        "  optimizer = Adam(parameters, lr=args.learning_rate*args.batch_size/256)\r\n",
        "elif args.optim == 'sgd':\r\n",
        "  optimizer = SGD(parameters, lr=args.learning_rate*args.batch_size/256, momentum=0.9)\r\n",
        "else: # default is LARS\r\n",
        "  optimizer = LARS(byol.named_modules(), lr=args.learning_rate*args.batch_size/256, weight_decay=args.weight_decay)\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3j31r4xYs36"
      },
      "source": [
        "### knn monitor define ###\r\n",
        "from tqdm import tqdm\r\n",
        "import torch.nn.functional as F \r\n",
        "# # code copied from https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb#scrollTo=RI1Y8bSImD7N\r\n",
        "# # test using a knn monitor\r\n",
        "# def knn_monitor(net, memory_data_loader, test_data_loader, k=200, t=0.1, hide_progress=False):\r\n",
        "#   net.eval()\r\n",
        "#   classes = len(memory_data_loader.dataset.classes)\r\n",
        "#   total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\r\n",
        "#   with torch.no_grad():\r\n",
        "#     # generate feature bank\r\n",
        "#     for data, target in tqdm(memory_data_loader, desc='Feature extracting', leave=False, disable=hide_progress):\r\n",
        "#       feature = net(data.cuda(non_blocking=True))\r\n",
        "#       feature = F.normalize(feature, dim=1)\r\n",
        "#       feature_bank.append(feature)\r\n",
        "#     # [D, N]\r\n",
        "#     feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\r\n",
        "#     # [N]\r\n",
        "#     feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\r\n",
        "#     # loop test data to predict the label by weighted knn search\r\n",
        "#     test_bar = tqdm(test_data_loader, desc='kNN', disable=hide_progress)\r\n",
        "#     for data, target in test_bar:\r\n",
        "#       data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\r\n",
        "#       feature = net(data)\r\n",
        "#       feature = F.normalize(feature, dim=1)\r\n",
        "      \r\n",
        "#       pred_labels = knn_predict(feature, feature_bank, feature_labels, classes, k, t)\r\n",
        "\r\n",
        "#       total_num += data.size(0)\r\n",
        "#       total_top1 += (pred_labels[:, 0] == target).float().sum().item()\r\n",
        "#       test_bar.set_postfix({'Accuracy':total_top1 / total_num * 100})\r\n",
        "#   return total_top1 / total_num * 100\r\n",
        "\r\n",
        "# # knn monitor as in InstDisc https://arxiv.org/abs/1805.01978\r\n",
        "# # implementation follows http://github.com/zhirongw/lemniscate.pytorch and https://github.com/leftthomas/SimCLR\r\n",
        "# def knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t):\r\n",
        "#   # compute cos similarity between each feature vector and feature bank ---> [B, N]\r\n",
        "#   sim_matrix = torch.mm(feature, feature_bank)\r\n",
        "#   # [B, K]\r\n",
        "#   sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)\r\n",
        "#   # [B, K]\r\n",
        "#   sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)\r\n",
        "#   sim_weight = (sim_weight / knn_t).exp()\r\n",
        "\r\n",
        "#   # counts for each class\r\n",
        "#   one_hot_label = torch.zeros(feature.size(0) * knn_k, classes, device=sim_labels.device)\r\n",
        "#   # [B*K, C]\r\n",
        "#   one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\r\n",
        "#   # weighted score ---> [B, C]\r\n",
        "#   pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1), dim=1)\r\n",
        "\r\n",
        "#   pred_labels = pred_scores.argsort(dim=-1, descending=True)\r\n",
        "#   return pred_labels\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur2YMpd2Vi-U",
        "outputId": "d98acb12-9bf2-497b-ea11-30371da0d00b"
      },
      "source": [
        "### Training ###\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "from collections import defaultdict\r\n",
        "from datetime import datetime\r\n",
        "import os\r\n",
        "\r\n",
        "writer = SummaryWriter()\r\n",
        "\r\n",
        "global_step = 0\r\n",
        "for epoch in tqdm(range(0, args.num_epochs), desc=f'Training'):\r\n",
        "  metrics = defaultdict(list)\r\n",
        "  \r\n",
        "  for step, ((x1, x2), labels) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1+args.current_epochs}/{args.num_epochs}')):\r\n",
        "    x1, x2 = x1.cuda(non_blocking=True), x2.cuda(non_blocking=True)\r\n",
        "\r\n",
        "    main_loss = byol(x1, x2)\r\n",
        "    optimizer.zero_grad()\r\n",
        "    main_loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "    byol.module.update_moving_average(epoch+1+args.current_epochs, args.num_epochs)\r\n",
        "    \r\n",
        "    writer.add_scalar(\"Loss/train_step\", main_loss, global_step)\r\n",
        "    metrics[\"Loss/train\"].append(main_loss.item())\r\n",
        "    global_step += 1\r\n",
        "  \r\n",
        "  for k, v in metrics.items():\r\n",
        "    writer.add_scalar(k, np.array(v).mean(), epoch+1+args.current_epochs)\r\n",
        "\r\n",
        "  if epoch+1%args.checkpoint_epochs == 0:\r\n",
        "    ckpt_path = os.path.join(tmp_dir, f\"byol_{args.optim}_{epoch+1+args.current_epochs}.pt\")\r\n",
        "    print(f'Saving model at epoch {epoch+1+args.current_epochs}')\r\n",
        "    torch.save({\r\n",
        "        'epoch':epoch+1+args.current_epochs, \r\n",
        "        'state_dict':byol.module.state_dict()\r\n",
        "    }, ckpt_path)\r\n",
        "\r\n",
        "ckpt_path = os.path.join(tmp_dir, f\"byol_{args.optim}_final.pt\")\r\n",
        "print(f'Saving final model at epoch {epoch+1+args.current_epochs}')\r\n",
        "torch.save({\r\n",
        "    'epoch':epoch+1+args.current_epochs, \r\n",
        "    'state_dict':byol.module.state_dict()\r\n",
        "}, ckpt_path)\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Epoch 1/1:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Epoch 1/1:  25%|██▌       | 1/4 [00:00<00:02,  1.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Epoch 1/1:  50%|█████     | 2/4 [00:01<00:01,  1.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Epoch 1/1:  75%|███████▌  | 3/4 [00:01<00:00,  1.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Epoch 1/1: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s]\n",
            "\n",
            "\n",
            "Training: 100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving final model at epoch 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGIhkzWUQAaX"
      },
      "source": [
        "### knn check ###\r\n",
        "# main_accuracy = knn_monitor(byol.module.backbone, memory_loader, test_loader, k=min(200, len(memory_loader.dataset)), hide_progress=True)\r\n",
        "# print('Accuracy:', main_accuracy)\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dELIoWzCuZt4"
      },
      "source": [
        "### learning rate scheduler define ###\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class LR_Scheduler(object):\r\n",
        "  def __init__(self, optimizer, warmup_epochs, warmup_lr, num_epochs, base_lr, final_lr, iter_per_epoch, constant_predictor_lr=False):\r\n",
        "    self.base_lr = base_lr\r\n",
        "    self.constant_predictor_lr = constant_predictor_lr\r\n",
        "    warmup_iter = iter_per_epoch * warmup_epochs\r\n",
        "    warmup_lr_schedule = np.linspace(warmup_lr, base_lr, warmup_iter)\r\n",
        "    decay_iter = iter_per_epoch * (num_epochs - warmup_epochs)\r\n",
        "    cosine_lr_schedule = final_lr+0.5*(base_lr-final_lr)*(1+np.cos(np.pi*np.arange(decay_iter)/decay_iter))\r\n",
        "    \r\n",
        "    self.lr_schedule = np.concatenate((warmup_lr_schedule, cosine_lr_schedule))\r\n",
        "    self.optimizer = optimizer\r\n",
        "    self.iter = 0\r\n",
        "    self.current_lr = 0\r\n",
        "  def step(self):\r\n",
        "    for param_group in self.optimizer.param_groups:\r\n",
        "\r\n",
        "      if self.constant_predictor_lr and param_group['name'] == 'predictor':\r\n",
        "        param_group['lr'] = self.base_lr\r\n",
        "      else:\r\n",
        "        lr = param_group['lr'] = self.lr_schedule[self.iter]\r\n",
        "    \r\n",
        "    self.iter += 1\r\n",
        "    self.current_lr = lr\r\n",
        "    return lr\r\n",
        "  def get_lr(self):\r\n",
        "    return self.current_lr\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRogCvo2Wu_2"
      },
      "source": [
        "### Linear Evaluation define ###\r\n",
        "class AverageMeter():\r\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\r\n",
        "    def __init__(self, name, fmt=':f'):\r\n",
        "        self.name = name\r\n",
        "        self.fmt = fmt\r\n",
        "        self.log = []\r\n",
        "        self.val = 0\r\n",
        "        self.avg = 0\r\n",
        "        self.sum = 0\r\n",
        "        self.count = 0\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.log.append(self.avg)\r\n",
        "        self.val = 0\r\n",
        "        self.avg = 0\r\n",
        "        self.sum = 0\r\n",
        "        self.count = 0\r\n",
        "\r\n",
        "    def update(self, val, n=1):\r\n",
        "        self.val = val\r\n",
        "        self.sum += val * n\r\n",
        "        self.count += n\r\n",
        "        self.avg = self.sum / self.count\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\r\n",
        "        return fmtstr.format(**self.__dict__)\r\n",
        "\r\n",
        "def linear_eval(args, eval_from):\r\n",
        "  eval_train_loader = torch.utils.data.DataLoader(\r\n",
        "      torchvision.datasets.CIFAR10(\r\n",
        "          root=args.dataset_dir, \r\n",
        "          train=True, \r\n",
        "          download=False, \r\n",
        "          transform=Transform_single(size=args.image_size, train=True), \r\n",
        "      ), \r\n",
        "      shuffle=True,\r\n",
        "      batch_size=args.batch_size,\r\n",
        "      num_workers=args.num_workers,\r\n",
        "      drop_last=True,\r\n",
        "      pin_memory=True,\r\n",
        "  )\r\n",
        "\r\n",
        "  eval_test_loader = torch.utils.data.DataLoader(\r\n",
        "      torchvision.datasets.CIFAR10(\r\n",
        "          root=args.dataset_dir, \r\n",
        "          train=False, \r\n",
        "          download=False, \r\n",
        "          transform=Transform_single(size=args.image_size, train=False), \r\n",
        "      ), \r\n",
        "      shuffle=False,\r\n",
        "      batch_size=args.batch_size,\r\n",
        "      num_workers=args.num_workers,\r\n",
        "      drop_last=True,\r\n",
        "      pin_memory=True,\r\n",
        "  )\r\n",
        "\r\n",
        "  eval_model = eval(f\"{args.resnet_version}()\")\r\n",
        "  eval_model.output_dim = eval_model.fc.in_features\r\n",
        "  eval_model.fc = torch.nn.Identity()\r\n",
        "  eval_classifier = nn.Linear(in_features=eval_model.output_dim, out_features=10, bias=True).to(args.device)\r\n",
        "\r\n",
        "  ###\r\n",
        "  assert eval_from is not None\r\n",
        "  eval_save_dict = torch.load(eval_from, map_location='cuda')\r\n",
        "  eval_msg = eval_model.load_state_dict({k[9:]:v for k, v in eval_save_dict['state_dict'].items() if k.startswith('backbone.')}, strict=True)\r\n",
        "  \r\n",
        "  print(eval_msg)\r\n",
        "  eval_model = eval_model.to(args.device)\r\n",
        "  eval_model = torch.nn.DataParallel(eval_model)\r\n",
        "\r\n",
        "  # if torch.cuda.device_count() > 1: eval_classifier = torch.nn.SyncBatchNorm.convert_sync_batchnorm(eval_classifier)\r\n",
        "  eval_classifier = torch.nn.DataParallel(eval_classifier)\r\n",
        "  # define optimizer 'sgd', eval_classifier, lr=eval_base_lr=30, momentum=eval_optim_momentum-0.9, weight_decay=eval_optim_weight_decay=0\r\n",
        "  predictor_prefix = ('module.predictor', 'predictor')\r\n",
        "  parameters = [{\r\n",
        "      'name': 'base',\r\n",
        "      'params': [param for name, param in eval_classifier.named_parameters() if not name.startswith(predictor_prefix)],\r\n",
        "      'lr': 30\r\n",
        "  },{\r\n",
        "      'name': 'predictor',\r\n",
        "      'params': [param for name, param in eval_classifier.named_parameters() if name.startswith(predictor_prefix)],\r\n",
        "      'lr': 30\r\n",
        "  }]\r\n",
        "  eval_optimizer = torch.optim.SGD(parameters, lr=30, momentum=0.9, weight_decay=0)\r\n",
        "\r\n",
        "  # define lr scheduler\r\n",
        "  eval_lr_scheduler = LR_Scheduler(\r\n",
        "      eval_optimizer,\r\n",
        "      0, 0*args.batch_size/256, \r\n",
        "      30, 30*args.batch_size/256, 0*args.batch_size/256, \r\n",
        "      len(eval_train_loader),\r\n",
        "  )\r\n",
        "\r\n",
        "  eval_loss_meter = AverageMeter(name='Loss')\r\n",
        "  eval_acc_meter = AverageMeter(name='Accuracy')\r\n",
        "\r\n",
        "  # Start training\r\n",
        "  eval_global_progress = tqdm(range(0, args.eval_epochs), desc=f'Evaluating')\r\n",
        "  for epoch in eval_global_progress:\r\n",
        "    eval_loss_meter.reset()\r\n",
        "    eval_model.eval()\r\n",
        "    eval_classifier.train()\r\n",
        "    eval_local_progress = tqdm(eval_train_loader, desc=f'Epoch {epoch}/{args.eval_epochs}', disable=True)\r\n",
        "    \r\n",
        "    for idx, (images, labels) in enumerate(eval_local_progress):\r\n",
        "\r\n",
        "      eval_classifier.zero_grad()\r\n",
        "      with torch.no_grad():\r\n",
        "        eval_feature = eval_model(images.to(args.device))\r\n",
        "\r\n",
        "      eval_preds = eval_classifier(eval_feature)\r\n",
        "\r\n",
        "      eval_loss = F.cross_entropy(eval_preds, labels.to(args.device))\r\n",
        "\r\n",
        "      eval_loss.backward()\r\n",
        "      eval_optimizer.step()\r\n",
        "      eval_loss_meter.update(eval_loss.item())\r\n",
        "      eval_lr = eval_lr_scheduler.step()\r\n",
        "      eval_local_progress.set_postfix({'lr':eval_lr, \"loss\":eval_loss_meter.val, 'loss_avg':eval_loss_meter.avg})\r\n",
        "\r\n",
        "  eval_classifier.eval()\r\n",
        "  eval_correct, eval_total = 0, 0\r\n",
        "  eval_acc_meter.reset()\r\n",
        "  for idx, (images, labels) in enumerate(eval_test_loader):\r\n",
        "    with torch.no_grad():\r\n",
        "      eval_feature = eval_model(images.to(args.device))\r\n",
        "      eval_preds = eval_classifier(eval_feature).argmax(dim=1)\r\n",
        "      eval_correct = (eval_preds == labels.to(args.device)).sum().item()\r\n",
        "      eval_acc_meter.update(eval_correct/eval_preds.shape[0])\r\n",
        "  print(f'Accuracy = {eval_acc_meter.avg*100:.2f}')\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QvV0X8xaDri",
        "outputId": "f767f424-dafe-4d72-a61e-e3ef6c8e12c2"
      },
      "source": [
        "### liner evaluation ###\r\n",
        "if args.eval is not False:\r\n",
        "  linear_eval(args, ckpt_path)\r\n",
        "### ------------------------------------------ ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<All keys matched successfully>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Evaluating:   3%|▎         | 1/30 [53:28<25:50:38, 3208.22s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}