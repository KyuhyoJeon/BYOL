{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BYOL-spijkervet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdqCG2ivZ72bWEjvAFTnSu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyuhyoJeon/BYOL/blob/master/BYOL_spijkervet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC9f89FOm5XK"
      },
      "source": [
        "https://github.com/Spijkervet/BYOL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv4RETSsn3ef"
      },
      "source": [
        "import easydict\r\n",
        "import os\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "args = easydict.EasyDict({\r\n",
        "    'image_size':224, \r\n",
        "    'learning_rate':3e-4, \r\n",
        "    'momentum':None, \r\n",
        "    'weight_decay':1.5e-6, \r\n",
        "    'batch_size':256, \r\n",
        "    'num_epochs':100, \r\n",
        "    'resnet_version':'resnet18', \r\n",
        "    'optim':'adam', # 'lars', 'adam', 'sgd' \r\n",
        "    'checkpoint_epochs':10, \r\n",
        "    'dataset_dir':'./datasets', \r\n",
        "    'ckpt_dir':'./ckpt', \r\n",
        "    'num_workers':8, \r\n",
        "    'nodes':1, \r\n",
        "    'gpus':1, \r\n",
        "    'nr':0, \r\n",
        "    'device':'cuda', \r\n",
        "    'eval':True, \r\n",
        "    'dryrun':True, \r\n",
        "    'debug':False\r\n",
        "})\r\n",
        "\r\n",
        "if args.dryrun:\r\n",
        "  args.image_size=32\r\n",
        "  args.num_epochs = 10\r\n",
        "  args.batch_size = 256\r\n",
        "  args.num_workers = 0\r\n",
        "  args.dryrun_subset_size = 100\r\n",
        "  args.resnet_version = 'resnet18'\r\n",
        "\r\n",
        "if args.debug:\r\n",
        "  args.image_size=32\r\n",
        "  args.num_epochs = 1\r\n",
        "  args.batch_size = 2\r\n",
        "  args.num_workers = 0\r\n",
        "  args.debug_subset_size = 8\r\n",
        "  args.resnet_version = 'resnet18'\r\n",
        "\r\n",
        "tmp_dir = os.path.join(args.ckpt_dir, f\"{args.resnet_version}\", f\"{args.optim}\", f\"{datetime.now().strftime('%m%d%H')}\")\r\n",
        "if not os.path.exists(tmp_dir):\r\n",
        "  os.makedirs(tmp_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGMIEdOYnSl6"
      },
      "source": [
        "import torchvision\r\n",
        "\r\n",
        "class TransformsSimCLR:\r\n",
        "  \"\"\"\r\n",
        "  A stochastic data augmentation module that transforms any given data example randomly \r\n",
        "  resulting in two correlated views of the same example,\r\n",
        "  denoted x ̃i and x ̃j, which we consider as a positive pair.\r\n",
        "  \"\"\"\r\n",
        "  imagenet_mean_std = [[0.485, 0.456, 0.406],[0.229, 0.224, 0.225]]\r\n",
        "\r\n",
        "  def __init__(self, size, mean_std=imagenet_mean_std):\r\n",
        "    s = 1\r\n",
        "    color_jitter = torchvision.transforms.ColorJitter(\r\n",
        "        0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s\r\n",
        "    )\r\n",
        "    self.train_transform = torchvision.transforms.Compose(\r\n",
        "        [\r\n",
        "         torchvision.transforms.RandomResizedCrop(size=size),\r\n",
        "         torchvision.transforms.RandomHorizontalFlip(),  # with 0.5 probability\r\n",
        "         torchvision.transforms.RandomApply([color_jitter], p=0.8),\r\n",
        "         torchvision.transforms.RandomGrayscale(p=0.2),\r\n",
        "         torchvision.transforms.ToTensor(),\r\n",
        "         torchvision.transforms.Normalize(*mean_std)\r\n",
        "        ]\r\n",
        "    )\r\n",
        "\r\n",
        "    self.test_transform = torchvision.transforms.Compose(\r\n",
        "        [\r\n",
        "         torchvision.transforms.Resize(size=size),\r\n",
        "         torchvision.transforms.ToTensor(),\r\n",
        "         torchvision.transforms.Normalize(*mean_std)\r\n",
        "        ]\r\n",
        "    )\r\n",
        "\r\n",
        "  def __call__(self, x):\r\n",
        "    return self.train_transform(x), self.train_transform(x)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-g-vx_Gnmb4",
        "outputId": "d1adba4d-3356-443c-8078-fa1db74c0ce4"
      },
      "source": [
        "from torchvision import datasets\r\n",
        "import torch\r\n",
        "\r\n",
        "# dataset\r\n",
        "train_dataset = datasets.CIFAR10(\r\n",
        "    args.dataset_dir,\r\n",
        "    download=True,\r\n",
        "    transform=TransformsSimCLR(size=args.image_size) # paper 224\r\n",
        ")\r\n",
        "\r\n",
        "if args.debug:\r\n",
        "  train_dataset = torch.utils.data.Subset(train_dataset, range(0, args.debug_subset_size))\r\n",
        "  train_dataset.classes = train_dataset.dataset.classes\r\n",
        "  train_dataset.targets = train_dataset.dataset.targets\r\n",
        "\r\n",
        "# if args.dryrun:\r\n",
        "#   train_dataset = torch.utils.data.Subset(train_dataset, range(0, args.dryrun_subset_size))\r\n",
        "#   train_dataset.classes = train_dataset.dataset.classes\r\n",
        "#   train_dataset.targets = train_dataset.dataset.targets\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(\r\n",
        "    train_dataset,\r\n",
        "    batch_size=args.batch_size,\r\n",
        "    shuffle=True, \r\n",
        "    drop_last=True,\r\n",
        "    num_workers=args.num_workers,\r\n",
        "    pin_memory=True\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOkBEubppP0o"
      },
      "source": [
        "import copy\r\n",
        "import random\r\n",
        "from functools import wraps\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "# helper functions\r\n",
        "\r\n",
        "\r\n",
        "def default(val, def_val):\r\n",
        "  return def_val if val is None else val\r\n",
        "\r\n",
        "\r\n",
        "def flatten(t):\r\n",
        "  return t.reshape(t.shape[0], -1)\r\n",
        "\r\n",
        "\r\n",
        "def singleton(cache_key):\r\n",
        "  def inner_fn(fn):\r\n",
        "    @wraps(fn)\r\n",
        "    def wrapper(self, *args, **kwargs):\r\n",
        "      instance = getattr(self, cache_key)\r\n",
        "      if instance is not None:\r\n",
        "          return instance\r\n",
        "\r\n",
        "      instance = fn(self, *args, **kwargs)\r\n",
        "      setattr(self, cache_key, instance)\r\n",
        "      return instance\r\n",
        "\r\n",
        "    return wrapper\r\n",
        "\r\n",
        "  return inner_fn\r\n",
        "\r\n",
        "\r\n",
        "# loss fn\r\n",
        "\r\n",
        "\r\n",
        "def loss_fn(x, y):\r\n",
        "  x = F.normalize(x, dim=-1, p=2)\r\n",
        "  y = F.normalize(y, dim=-1, p=2)\r\n",
        "  return 2 - 2 * (x * y).sum(dim=-1)\r\n",
        "\r\n",
        "\r\n",
        "# augmentation utils\r\n",
        "\r\n",
        "\r\n",
        "class RandomApply(nn.Module):\r\n",
        "  def __init__(self, fn, p):\r\n",
        "    super().__init__()\r\n",
        "    self.fn = fn\r\n",
        "    self.p = p\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    if random.random() > self.p:\r\n",
        "      return x\r\n",
        "    return self.fn(x)\r\n",
        "\r\n",
        "\r\n",
        "# exponential moving average\r\n",
        "\r\n",
        "\r\n",
        "class EMA:\r\n",
        "  def __init__(self, beta):\r\n",
        "    super().__init__()\r\n",
        "    self.beta = beta\r\n",
        "\r\n",
        "  def update_average(self, old, new):\r\n",
        "    if old is None:\r\n",
        "      return new\r\n",
        "    return old * self.beta + (1 - self.beta) * new\r\n",
        "\r\n",
        "\r\n",
        "def update_moving_average(ema_updater, ma_model, current_model):\r\n",
        "  for current_params, ma_params in zip(\r\n",
        "      current_model.parameters(), ma_model.parameters()\r\n",
        "  ):\r\n",
        "    old_weight, up_weight = ma_params.data, current_params.data\r\n",
        "    ma_params.data = ema_updater.update_average(old_weight, up_weight)\r\n",
        "\r\n",
        "\r\n",
        "# MLP class for projector and predictor\r\n",
        "\r\n",
        "\r\n",
        "class MLP(nn.Module):\r\n",
        "  def __init__(self, dim, projection_size, hidden_size=4096):\r\n",
        "    super().__init__()\r\n",
        "    self.net = nn.Sequential(\r\n",
        "        nn.Linear(dim, hidden_size),\r\n",
        "        nn.BatchNorm1d(hidden_size),\r\n",
        "        nn.ReLU(inplace=True),\r\n",
        "        nn.Linear(hidden_size, projection_size),\r\n",
        "    )\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    return self.net(x)\r\n",
        "\r\n",
        "\r\n",
        "# a wrapper class for the base neural network\r\n",
        "# will manage the interception of the hidden layer output\r\n",
        "# and pipe it into the projecter and predictor nets\r\n",
        "\r\n",
        "\r\n",
        "class NetWrapper(nn.Module):\r\n",
        "  def __init__(self, net, projection_size, projection_hidden_size, layer=-2):\r\n",
        "    super().__init__()\r\n",
        "    self.net = net\r\n",
        "    self.layer = layer\r\n",
        "\r\n",
        "    self.projector = None\r\n",
        "    self.projection_size = projection_size\r\n",
        "    self.projection_hidden_size = projection_hidden_size\r\n",
        "\r\n",
        "    self.hidden = None\r\n",
        "    self.hook_registered = False\r\n",
        "\r\n",
        "  def _find_layer(self):\r\n",
        "    if type(self.layer) == str:\r\n",
        "      modules = dict([*self.net.named_modules()])\r\n",
        "      return modules.get(self.layer, None)\r\n",
        "    elif type(self.layer) == int:\r\n",
        "      children = [*self.net.children()]\r\n",
        "      return children[self.layer]\r\n",
        "    return None\r\n",
        "\r\n",
        "  def _hook(self, _, __, output):\r\n",
        "    self.hidden = flatten(output)\r\n",
        "\r\n",
        "  def _register_hook(self):\r\n",
        "    layer = self._find_layer()\r\n",
        "    assert layer is not None, f\"hidden layer ({self.layer}) not found\"\r\n",
        "    handle = layer.register_forward_hook(self._hook)\r\n",
        "    self.hook_registered = True\r\n",
        "\r\n",
        "  @singleton(\"projector\")\r\n",
        "  def _get_projector(self, hidden):\r\n",
        "    _, dim = hidden.shape\r\n",
        "    projector = MLP(dim, self.projection_size, self.projection_hidden_size)\r\n",
        "    return projector.to(hidden)\r\n",
        "\r\n",
        "  def get_representation(self, x):\r\n",
        "    if not self.hook_registered:\r\n",
        "      self._register_hook()\r\n",
        "\r\n",
        "    if self.layer == -1:\r\n",
        "      return self.net(x)\r\n",
        "\r\n",
        "    _ = self.net(x)\r\n",
        "    hidden = self.hidden\r\n",
        "    self.hidden = None\r\n",
        "    assert hidden is not None, f\"hidden layer {self.layer} never emitted an output\"\r\n",
        "    return hidden\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    representation = self.get_representation(x)\r\n",
        "    projector = self._get_projector(representation)\r\n",
        "    projection = projector(representation)\r\n",
        "    return projection\r\n",
        "\r\n",
        "\r\n",
        "# main class\r\n",
        "\r\n",
        "\r\n",
        "class BYOL(nn.Module):\r\n",
        "  def __init__(\r\n",
        "      self,\r\n",
        "      net,\r\n",
        "      image_size,\r\n",
        "      hidden_layer=-2,\r\n",
        "      projection_size=256,\r\n",
        "      projection_hidden_size=4096,\r\n",
        "      augment_fn=None,\r\n",
        "      moving_average_decay=0.99,\r\n",
        "  ):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.online_encoder = NetWrapper(\r\n",
        "        net, projection_size, projection_hidden_size, layer=hidden_layer\r\n",
        "    )\r\n",
        "    self.target_encoder = None\r\n",
        "    self.target_ema_updater = EMA(moving_average_decay)\r\n",
        "\r\n",
        "    self.online_predictor = MLP(\r\n",
        "        projection_size, projection_size, projection_hidden_size\r\n",
        "    )\r\n",
        "\r\n",
        "    # send a mock image tensor to instantiate singleton parameters\r\n",
        "    self.forward(torch.randn(2, 3, image_size, image_size), torch.randn(2, 3, image_size, image_size))\r\n",
        "\r\n",
        "  @singleton(\"target_encoder\")\r\n",
        "  def _get_target_encoder(self):\r\n",
        "    target_encoder = copy.deepcopy(self.online_encoder)\r\n",
        "    return target_encoder\r\n",
        "\r\n",
        "  def reset_moving_average(self):\r\n",
        "    del self.target_encoder\r\n",
        "    self.target_encoder = None\r\n",
        "\r\n",
        "  def update_moving_average(self):\r\n",
        "    assert (\r\n",
        "        self.target_encoder is not None\r\n",
        "    ), \"target encoder has not been created yet\"\r\n",
        "    update_moving_average(\r\n",
        "        self.target_ema_updater, self.target_encoder, self.online_encoder\r\n",
        "    )\r\n",
        "\r\n",
        "  def forward(self, image_one, image_two):\r\n",
        "    online_proj_one = self.online_encoder(image_one)\r\n",
        "    online_proj_two = self.online_encoder(image_two)\r\n",
        "\r\n",
        "    online_pred_one = self.online_predictor(online_proj_one)\r\n",
        "    online_pred_two = self.online_predictor(online_proj_two)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "      target_encoder = self._get_target_encoder()\r\n",
        "      target_proj_one = target_encoder(image_one)\r\n",
        "      target_proj_two = target_encoder(image_two)\r\n",
        "\r\n",
        "    loss_one = loss_fn(online_pred_one, target_proj_two.detach())\r\n",
        "    loss_two = loss_fn(online_pred_two, target_proj_one.detach())\r\n",
        "\r\n",
        "    loss = loss_one + loss_two\r\n",
        "    return loss.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYIhf23WpHQB"
      },
      "source": [
        "from torchvision import models\r\n",
        "\r\n",
        "# model\r\n",
        "if args.resnet_version == \"resnet18\":\r\n",
        "  resnet = models.resnet18(pretrained=False)\r\n",
        "elif args.resnet_version == \"resnet50\":\r\n",
        "  resnet = models.resnet50(pretrained=False)\r\n",
        "else:\r\n",
        "  raise NotImplementedError(\"ResNet not implemented\")\r\n",
        "\r\n",
        "model = BYOL(resnet, image_size=args.image_size, hidden_layer=\"avgpool\")\r\n",
        "model = model.cuda()\r\n",
        "model = torch.nn.DataParallel(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j7qiZ8BZvmG"
      },
      "source": [
        "from torch.optim import Adam, SGD\r\n",
        "from torch.optim.optimizer import Optimizer\r\n",
        "\r\n",
        "class LARS(Optimizer):\r\n",
        "  def __init__(self, named_modules, lr, momentum=0.9, trust_coef=1e-3, weight_decay=1.5e-6, exclude_bias_from_adaption=True):\r\n",
        "    defaults = dict(momentum=momentum, lr=lr, weight_decay=weight_decay, trust_coef=trust_coef)\r\n",
        "    parameters = self.exclude_from_model(named_modules, exclude_bias_from_adaption)\r\n",
        "    super(LARS, self).__init__(parameters, defaults)\r\n",
        "\r\n",
        "  @torch.no_grad() \r\n",
        "  def step(self):\r\n",
        "    for group in self.param_groups: # only 1 group in most cases \r\n",
        "      weight_decay = group['weight_decay']\r\n",
        "      momentum = group['momentum']\r\n",
        "      lr = group['lr']\r\n",
        "      trust_coef = group['trust_coef']\r\n",
        "      # print(group['name'])\r\n",
        "      # eps = group['eps']\r\n",
        "      for p in group['params']:\r\n",
        "        # breakpoint()\r\n",
        "        if p.grad is None:\r\n",
        "          continue\r\n",
        "        global_lr = lr\r\n",
        "        velocity = self.state[p].get('velocity', 0)  \r\n",
        "        # if name in self.exclude_from_layer_adaptation:\r\n",
        "        if self._use_weight_decay(group):\r\n",
        "          p.grad.data += weight_decay * p.data \r\n",
        "\r\n",
        "        trust_ratio = 1.0 \r\n",
        "        if self._do_layer_adaptation(group):\r\n",
        "          w_norm = torch.norm(p.data, p=2)\r\n",
        "          g_norm = torch.norm(p.grad.data, p=2)\r\n",
        "          trust_ratio = trust_coef * w_norm / g_norm if w_norm > 0 and g_norm > 0 else 1.0 \r\n",
        "        scaled_lr = global_lr * trust_ratio # trust_ratio is the local_lr \r\n",
        "        next_v = momentum * velocity + scaled_lr * p.grad.data \r\n",
        "        update = next_v\r\n",
        "        p.data = p.data - update \r\n",
        "\r\n",
        "  def _use_weight_decay(self, group):\r\n",
        "    return False if group['name'] == 'exclude' else True\r\n",
        "  def _do_layer_adaptation(self, group):\r\n",
        "    return False if group['name'] == 'exclude' else True\r\n",
        "\r\n",
        "  def exclude_from_model(self, named_modules, exclude_bias_from_adaption=True):\r\n",
        "    base = [] \r\n",
        "    exclude = []\r\n",
        "    for name, module in named_modules:\r\n",
        "      if type(module) in [nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]:\r\n",
        "        # if isinstance(module, torch.nn.modules.batchnorm._BatchNorm)\r\n",
        "        for name2, param in module.named_parameters():\r\n",
        "          exclude.append(param)\r\n",
        "      else:\r\n",
        "        for name2, param in module.named_parameters():\r\n",
        "          if name2 == 'bias':\r\n",
        "            exclude.append(param)\r\n",
        "          elif name2 == 'weight':\r\n",
        "            base.append(param)\r\n",
        "          else:\r\n",
        "            pass # non leaf modules \r\n",
        "    return [{\r\n",
        "        'name': 'base',\r\n",
        "        'params': base\r\n",
        "        },{\r\n",
        "        'name': 'exclude',\r\n",
        "        'params': exclude\r\n",
        "    }] if exclude_bias_from_adaption == True else [{\r\n",
        "        'name': 'base',\r\n",
        "        'params': base+exclude \r\n",
        "    }]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8ujLloRrL2q"
      },
      "source": [
        "# optimizer\r\n",
        "if args.optim == 'lars':\r\n",
        "  optimizer = LARS(model.named_modules(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\r\n",
        "elif args.optim == 'adam':\r\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\r\n",
        "elif args.optim == 'sgd':\r\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, momentum=0.9)\r\n",
        "else: # default = adam\r\n",
        "  optimizer = LARS(model.named_modules(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-FYYo_LrpHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3418efc2-f33a-4d15-ef8a-a98a202bb678"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "from collections import defaultdict\r\n",
        "from tqdm import tqdm\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "writer = SummaryWriter()\r\n",
        "\r\n",
        "\r\n",
        "global_step = 0\r\n",
        "for epoch in tqdm(range(args.num_epochs), desc=f'Training'):\r\n",
        "  metrics = defaultdict(list)\r\n",
        "  for step, ((x_i, x_j), _) in enumerate(train_loader):\r\n",
        "    x_i = x_i.cuda(non_blocking=True)\r\n",
        "    x_j = x_j.cuda(non_blocking=True)\r\n",
        "\r\n",
        "    loss = model(x_i, x_j)\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "    model.module.update_moving_average()  # update moving average of target encoder\r\n",
        "\r\n",
        "    # if step % 1 == 0:\r\n",
        "    #   print(f\"Step [{step}/{len(train_loader)}]:\\tLoss: {loss.item()}\")\r\n",
        "\r\n",
        "    writer.add_scalar(\"Loss/train_step\", loss, global_step)\r\n",
        "    metrics[\"Loss/train\"].append(loss.item())\r\n",
        "    global_step += 1\r\n",
        "\r\n",
        "  # write metrics to TensorBoard\r\n",
        "  for k, v in metrics.items():\r\n",
        "    writer.add_scalar(k, np.array(v).mean(), epoch)\r\n",
        "\r\n",
        "  if epoch % args.checkpoint_epochs == 0:\r\n",
        "    ckpt_path = os.path.join(tmp_dir, f\"byol1_{args.optim}_{epoch}.pt\")\r\n",
        "    print(f\"Saving model at epoch {epoch}\")\r\n",
        "    torch.save(resnet.state_dict(), ckpt_path)\r\n",
        "\r\n",
        "    # let other workers wait until model is finished\r\n",
        "    # dist.barrier()\r\n",
        "\r\n",
        "# save your improved network\r\n",
        "ckpt_path = os.path.join(tmp_dir, f\"byol1_{args.optim}_final.pt\")\r\n",
        "torch.save(resnet.state_dict(), ckpt_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "Training:  10%|█         | 1/10 [02:20<21:07, 140.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving model at epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training:  20%|██        | 2/10 [04:41<18:45, 140.64s/it]\u001b[A\n",
            "Training:  30%|███       | 3/10 [07:02<16:26, 140.87s/it]\u001b[A\n",
            "Training:  40%|████      | 4/10 [09:25<14:08, 141.46s/it]\u001b[A\n",
            "Training:  50%|█████     | 5/10 [11:47<11:48, 141.63s/it]\u001b[A\n",
            "Training:  60%|██████    | 6/10 [14:09<09:26, 141.72s/it]\u001b[A\n",
            "Training:  70%|███████   | 7/10 [16:30<07:04, 141.54s/it]\u001b[A\n",
            "Training:  80%|████████  | 8/10 [18:51<04:42, 141.43s/it]\u001b[A\n",
            "Training:  90%|█████████ | 9/10 [21:12<02:21, 141.33s/it]\u001b[A\n",
            "Training: 100%|██████████| 10/10 [23:32<00:00, 141.24s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH8qM0-Y1mKK"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "import torch.nn.functional as F \r\n",
        "# code copied from https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb#scrollTo=RI1Y8bSImD7N\r\n",
        "# test using a knn monitor\r\n",
        "def knn_monitor(net, memory_data_loader, test_data_loader, k=200, t=0.1, hide_progress=False):\r\n",
        "  net.eval()\r\n",
        "  classes = len(memory_data_loader.dataset.classes)\r\n",
        "  total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\r\n",
        "  with torch.no_grad():\r\n",
        "    # generate feature bank\r\n",
        "    for data, target in tqdm(memory_data_loader, desc='Feature extracting', leave=False, disable=hide_progress):\r\n",
        "      feature = net(data.cuda(non_blocking=True))\r\n",
        "      feature = F.normalize(feature, dim=1)\r\n",
        "      feature_bank.append(feature)\r\n",
        "    # [D, N]\r\n",
        "    feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\r\n",
        "    # [N]\r\n",
        "    feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\r\n",
        "    # loop test data to predict the label by weighted knn search\r\n",
        "    test_bar = tqdm(test_data_loader, desc='kNN', disable=hide_progress)\r\n",
        "    for data, target in test_bar:\r\n",
        "      data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\r\n",
        "      feature = net(data)\r\n",
        "      feature = F.normalize(feature, dim=1)\r\n",
        "      \r\n",
        "      pred_labels = knn_predict(feature, feature_bank, feature_labels, classes, k, t)\r\n",
        "\r\n",
        "      total_num += data.size(0)\r\n",
        "      total_top1 += (pred_labels[:, 0] == target).float().sum().item()\r\n",
        "      test_bar.set_postfix({'Accuracy':total_top1 / total_num * 100})\r\n",
        "  return total_top1 / total_num * 100\r\n",
        "\r\n",
        "# knn monitor as in InstDisc https://arxiv.org/abs/1805.01978\r\n",
        "# implementation follows http://github.com/zhirongw/lemniscate.pytorch and https://github.com/leftthomas/SimCLR\r\n",
        "def knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t):\r\n",
        "  # compute cos similarity between each feature vector and feature bank ---> [B, N]\r\n",
        "  sim_matrix = torch.mm(feature, feature_bank)\r\n",
        "  # [B, K]\r\n",
        "  sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)\r\n",
        "  # [B, K]\r\n",
        "  sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)\r\n",
        "  sim_weight = (sim_weight / knn_t).exp()\r\n",
        "\r\n",
        "  # counts for each class\r\n",
        "  one_hot_label = torch.zeros(feature.size(0) * knn_k, classes, device=sim_labels.device)\r\n",
        "  # [B*K, C]\r\n",
        "  one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\r\n",
        "  # weighted score ---> [B, C]\r\n",
        "  pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1), dim=1)\r\n",
        "\r\n",
        "  pred_labels = pred_scores.argsort(dim=-1, descending=True)\r\n",
        "  return pred_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwpwMJWR2YRP"
      },
      "source": [
        "from PIL import Image\r\n",
        "\r\n",
        "class Transform_single():\r\n",
        "  imagenet_mean_std = [[0.485, 0.456, 0.406],[0.229, 0.224, 0.225]]\r\n",
        "  def __init__(self, size, train, normalize=imagenet_mean_std):\r\n",
        "  #def __init__(self, size, train):\r\n",
        "    if train == True:\r\n",
        "      self.transform = torchvision.transforms.Compose(\r\n",
        "          [\r\n",
        "           torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), \r\n",
        "                                        ratio=(3.0/4.0,4.0/3.0), \r\n",
        "                                        interpolation=Image.BICUBIC\r\n",
        "                                        ),\r\n",
        "           torchvision.transforms.RandomHorizontalFlip(),\r\n",
        "           torchvision.transforms.ToTensor(),\r\n",
        "           torchvision.transforms.Normalize(*normalize)\r\n",
        "          ]\r\n",
        "      )\r\n",
        "    else:\r\n",
        "      self.transform = torchvision.transforms.Compose(\r\n",
        "          [\r\n",
        "           torchvision.transforms.Resize(int(size*(8/7)), \r\n",
        "                             interpolation=Image.BICUBIC\r\n",
        "                             ), # 224 -> 256 \r\n",
        "           torchvision.transforms.CenterCrop(size),\r\n",
        "           torchvision.transforms.ToTensor(),\r\n",
        "           torchvision.transforms.Normalize(*normalize)\r\n",
        "          ]\r\n",
        "      )\r\n",
        "\r\n",
        "  def __call__(self, x):\r\n",
        "    return self.transform(x)\r\n",
        "\r\n",
        "memory_dataset = datasets.CIFAR10(\r\n",
        "    root=args.dataset_dir, \r\n",
        "    train=True, \r\n",
        "    download=False, \r\n",
        "    transform=Transform_single(size=args.image_size, train=False), \r\n",
        "    )\r\n",
        "memory_loader = torch.utils.data.DataLoader(\r\n",
        "    memory_dataset, \r\n",
        "    shuffle=False,\r\n",
        "    batch_size=args.batch_size,\r\n",
        "    num_workers=args.num_workers,\r\n",
        "    drop_last=True,\r\n",
        "    pin_memory=True,\r\n",
        "    )\r\n",
        "\r\n",
        "test_dataset = datasets.CIFAR10(\r\n",
        "    root=args.dataset_dir, \r\n",
        "    train=False, \r\n",
        "    download=False, \r\n",
        "    transform=Transform_single(size=args.image_size, train=False), \r\n",
        ")\r\n",
        "test_loader = torch.utils.data.DataLoader(\r\n",
        "    test_dataset, \r\n",
        "    shuffle=False,\r\n",
        "    batch_size=args.batch_size,\r\n",
        "    num_workers=args.num_workers,\r\n",
        "    drop_last=True,\r\n",
        "    pin_memory=True,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gk5FIO-1oLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70cfc101-0245-4c24-8786-ec8cabb1838d"
      },
      "source": [
        "accuracy = knn_monitor(model.module.online_encoder.net, memory_loader, test_loader, k=min(200, len(memory_loader.dataset)), hide_progress=True)\r\n",
        "print('Accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Feature extracting:   0%|          | 0/195 [00:00<?, ?it/s]\u001b[A\n",
            "Feature extracting:   1%|          | 1/195 [00:00<00:21,  9.07it/s]\u001b[A\n",
            "Feature extracting:   2%|▏         | 3/195 [00:00<00:19,  9.73it/s]\u001b[A\n",
            "Feature extracting:   3%|▎         | 5/195 [00:00<00:17, 10.59it/s]\u001b[A\n",
            "Feature extracting:   4%|▎         | 7/195 [00:00<00:17, 11.06it/s]\u001b[A\n",
            "Feature extracting:   5%|▍         | 9/195 [00:00<00:16, 11.54it/s]\u001b[A\n",
            "Feature extracting:   6%|▌         | 11/195 [00:00<00:15, 12.10it/s]\u001b[A\n",
            "Feature extracting:   7%|▋         | 13/195 [00:01<00:14, 12.45it/s]\u001b[A\n",
            "Feature extracting:   8%|▊         | 15/195 [00:01<00:14, 12.65it/s]\u001b[A\n",
            "Feature extracting:   9%|▊         | 17/195 [00:01<00:14, 12.62it/s]\u001b[A\n",
            "Feature extracting:  10%|▉         | 19/195 [00:01<00:13, 12.63it/s]\u001b[A\n",
            "Feature extracting:  11%|█         | 21/195 [00:01<00:13, 12.62it/s]\u001b[A\n",
            "Feature extracting:  12%|█▏        | 23/195 [00:01<00:13, 12.48it/s]\u001b[A\n",
            "Feature extracting:  13%|█▎        | 25/195 [00:02<00:14, 11.81it/s]\u001b[A\n",
            "Feature extracting:  14%|█▍        | 27/195 [00:02<00:14, 11.84it/s]\u001b[A\n",
            "Feature extracting:  15%|█▍        | 29/195 [00:02<00:13, 11.96it/s]\u001b[A\n",
            "Feature extracting:  16%|█▌        | 31/195 [00:02<00:13, 12.25it/s]\u001b[A\n",
            "Feature extracting:  17%|█▋        | 33/195 [00:02<00:12, 12.53it/s]\u001b[A\n",
            "Feature extracting:  18%|█▊        | 35/195 [00:02<00:12, 12.79it/s]\u001b[A\n",
            "Feature extracting:  19%|█▉        | 37/195 [00:02<00:12, 13.03it/s]\u001b[A\n",
            "Feature extracting:  20%|██        | 39/195 [00:03<00:12, 12.63it/s]\u001b[A\n",
            "Feature extracting:  21%|██        | 41/195 [00:03<00:12, 12.82it/s]\u001b[A\n",
            "Feature extracting:  22%|██▏       | 43/195 [00:03<00:11, 12.98it/s]\u001b[A\n",
            "Feature extracting:  23%|██▎       | 45/195 [00:03<00:12, 12.48it/s]\u001b[A\n",
            "Feature extracting:  24%|██▍       | 47/195 [00:03<00:11, 12.68it/s]\u001b[A\n",
            "Feature extracting:  25%|██▌       | 49/195 [00:03<00:11, 12.98it/s]\u001b[A\n",
            "Feature extracting:  26%|██▌       | 51/195 [00:04<00:11, 12.63it/s]\u001b[A\n",
            "Feature extracting:  27%|██▋       | 53/195 [00:04<00:11, 12.75it/s]\u001b[A\n",
            "Feature extracting:  28%|██▊       | 55/195 [00:04<00:10, 12.78it/s]\u001b[A\n",
            "Feature extracting:  29%|██▉       | 57/195 [00:04<00:10, 12.91it/s]\u001b[A\n",
            "Feature extracting:  30%|███       | 59/195 [00:04<00:10, 12.83it/s]\u001b[A\n",
            "Feature extracting:  31%|███▏      | 61/195 [00:04<00:10, 12.69it/s]\u001b[A\n",
            "Feature extracting:  32%|███▏      | 63/195 [00:05<00:10, 12.85it/s]\u001b[A\n",
            "Feature extracting:  33%|███▎      | 65/195 [00:05<00:10, 12.87it/s]\u001b[A\n",
            "Feature extracting:  34%|███▍      | 67/195 [00:05<00:09, 13.03it/s]\u001b[A\n",
            "Feature extracting:  35%|███▌      | 69/195 [00:05<00:09, 13.04it/s]\u001b[A\n",
            "Feature extracting:  36%|███▋      | 71/195 [00:05<00:09, 12.77it/s]\u001b[A\n",
            "Feature extracting:  37%|███▋      | 73/195 [00:05<00:09, 13.07it/s]\u001b[A\n",
            "Feature extracting:  38%|███▊      | 75/195 [00:05<00:09, 13.28it/s]\u001b[A\n",
            "Feature extracting:  39%|███▉      | 77/195 [00:06<00:09, 12.99it/s]\u001b[A\n",
            "Feature extracting:  41%|████      | 79/195 [00:06<00:09, 12.88it/s]\u001b[A\n",
            "Feature extracting:  42%|████▏     | 81/195 [00:06<00:08, 12.99it/s]\u001b[A\n",
            "Feature extracting:  43%|████▎     | 83/195 [00:06<00:08, 13.12it/s]\u001b[A\n",
            "Feature extracting:  44%|████▎     | 85/195 [00:06<00:08, 13.23it/s]\u001b[A\n",
            "Feature extracting:  45%|████▍     | 87/195 [00:06<00:08, 12.99it/s]\u001b[A\n",
            "Feature extracting:  46%|████▌     | 89/195 [00:07<00:08, 12.84it/s]\u001b[A\n",
            "Feature extracting:  47%|████▋     | 91/195 [00:07<00:08, 12.85it/s]\u001b[A\n",
            "Feature extracting:  48%|████▊     | 93/195 [00:07<00:08, 12.31it/s]\u001b[A\n",
            "Feature extracting:  49%|████▊     | 95/195 [00:07<00:08, 11.94it/s]\u001b[A\n",
            "Feature extracting:  50%|████▉     | 97/195 [00:07<00:07, 12.28it/s]\u001b[A\n",
            "Feature extracting:  51%|█████     | 99/195 [00:07<00:07, 12.61it/s]\u001b[A\n",
            "Feature extracting:  52%|█████▏    | 101/195 [00:07<00:07, 12.94it/s]\u001b[A\n",
            "Feature extracting:  53%|█████▎    | 103/195 [00:08<00:07, 13.03it/s]\u001b[A\n",
            "Feature extracting:  54%|█████▍    | 105/195 [00:08<00:06, 12.86it/s]\u001b[A\n",
            "Feature extracting:  55%|█████▍    | 107/195 [00:08<00:06, 12.57it/s]\u001b[A\n",
            "Feature extracting:  56%|█████▌    | 109/195 [00:08<00:06, 12.40it/s]\u001b[A\n",
            "Feature extracting:  57%|█████▋    | 111/195 [00:08<00:06, 12.65it/s]\u001b[A\n",
            "Feature extracting:  58%|█████▊    | 113/195 [00:08<00:06, 12.85it/s]\u001b[A\n",
            "Feature extracting:  59%|█████▉    | 115/195 [00:09<00:06, 12.86it/s]\u001b[A\n",
            "Feature extracting:  60%|██████    | 117/195 [00:09<00:06, 12.78it/s]\u001b[A\n",
            "Feature extracting:  61%|██████    | 119/195 [00:09<00:05, 12.72it/s]\u001b[A\n",
            "Feature extracting:  62%|██████▏   | 121/195 [00:09<00:05, 12.48it/s]\u001b[A\n",
            "Feature extracting:  63%|██████▎   | 123/195 [00:09<00:05, 12.47it/s]\u001b[A\n",
            "Feature extracting:  64%|██████▍   | 125/195 [00:09<00:05, 12.32it/s]\u001b[A\n",
            "Feature extracting:  65%|██████▌   | 127/195 [00:10<00:05, 12.13it/s]\u001b[A\n",
            "Feature extracting:  66%|██████▌   | 129/195 [00:10<00:05, 11.75it/s]\u001b[A\n",
            "Feature extracting:  67%|██████▋   | 131/195 [00:10<00:05, 12.04it/s]\u001b[A\n",
            "Feature extracting:  68%|██████▊   | 133/195 [00:10<00:05, 12.35it/s]\u001b[A\n",
            "Feature extracting:  69%|██████▉   | 135/195 [00:10<00:04, 12.63it/s]\u001b[A\n",
            "Feature extracting:  70%|███████   | 137/195 [00:10<00:04, 12.72it/s]\u001b[A\n",
            "Feature extracting:  71%|███████▏  | 139/195 [00:10<00:04, 12.86it/s]\u001b[A\n",
            "Feature extracting:  72%|███████▏  | 141/195 [00:11<00:04, 12.94it/s]\u001b[A\n",
            "Feature extracting:  73%|███████▎  | 143/195 [00:11<00:04, 12.75it/s]\u001b[A\n",
            "Feature extracting:  74%|███████▍  | 145/195 [00:11<00:04, 12.17it/s]\u001b[A\n",
            "Feature extracting:  75%|███████▌  | 147/195 [00:11<00:04, 11.91it/s]\u001b[A\n",
            "Feature extracting:  76%|███████▋  | 149/195 [00:11<00:03, 11.82it/s]\u001b[A\n",
            "Feature extracting:  77%|███████▋  | 151/195 [00:12<00:03, 11.79it/s]\u001b[A\n",
            "Feature extracting:  78%|███████▊  | 153/195 [00:12<00:03, 11.77it/s]\u001b[A\n",
            "Feature extracting:  79%|███████▉  | 155/195 [00:12<00:03, 12.07it/s]\u001b[A\n",
            "Feature extracting:  81%|████████  | 157/195 [00:12<00:03, 12.13it/s]\u001b[A\n",
            "Feature extracting:  82%|████████▏ | 159/195 [00:12<00:02, 12.02it/s]\u001b[A\n",
            "Feature extracting:  83%|████████▎ | 161/195 [00:12<00:02, 12.41it/s]\u001b[A\n",
            "Feature extracting:  84%|████████▎ | 163/195 [00:12<00:02, 12.46it/s]\u001b[A\n",
            "Feature extracting:  85%|████████▍ | 165/195 [00:13<00:02, 12.69it/s]\u001b[A\n",
            "Feature extracting:  86%|████████▌ | 167/195 [00:13<00:02, 12.71it/s]\u001b[A\n",
            "Feature extracting:  87%|████████▋ | 169/195 [00:13<00:02, 12.80it/s]\u001b[A\n",
            "Feature extracting:  88%|████████▊ | 171/195 [00:13<00:01, 12.82it/s]\u001b[A\n",
            "Feature extracting:  89%|████████▊ | 173/195 [00:13<00:01, 13.01it/s]\u001b[A\n",
            "Feature extracting:  90%|████████▉ | 175/195 [00:13<00:01, 13.02it/s]\u001b[A\n",
            "Feature extracting:  91%|█████████ | 177/195 [00:14<00:01, 13.21it/s]\u001b[A\n",
            "Feature extracting:  92%|█████████▏| 179/195 [00:14<00:01, 12.51it/s]\u001b[A\n",
            "Feature extracting:  93%|█████████▎| 181/195 [00:14<00:01, 12.70it/s]\u001b[A\n",
            "Feature extracting:  94%|█████████▍| 183/195 [00:14<00:00, 12.47it/s]\u001b[A\n",
            "Feature extracting:  95%|█████████▍| 185/195 [00:14<00:00, 12.63it/s]\u001b[A\n",
            "Feature extracting:  96%|█████████▌| 187/195 [00:14<00:00, 12.84it/s]\u001b[A\n",
            "Feature extracting:  97%|█████████▋| 189/195 [00:14<00:00, 12.91it/s]\u001b[A\n",
            "Feature extracting:  98%|█████████▊| 191/195 [00:15<00:00, 12.78it/s]\u001b[A\n",
            "Feature extracting:  99%|█████████▉| 193/195 [00:15<00:00, 12.24it/s]\u001b[A\n",
            "Feature extracting: 100%|██████████| 195/195 [00:15<00:00, 11.75it/s]\u001b[A\n",
            "                                                                     \u001b[A\n",
            "kNN:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "kNN:   0%|          | 0/39 [00:00<?, ?it/s, Accuracy=38.3]\u001b[A\n",
            "kNN:   3%|▎         | 1/39 [00:00<00:04,  7.65it/s, Accuracy=38.3]\u001b[A\n",
            "kNN:   3%|▎         | 1/39 [00:00<00:04,  7.65it/s, Accuracy=37.3]\u001b[A\n",
            "kNN:   5%|▌         | 2/39 [00:00<00:04,  7.53it/s, Accuracy=37.3]\u001b[A\n",
            "kNN:   5%|▌         | 2/39 [00:00<00:04,  7.53it/s, Accuracy=37.2]\u001b[A\n",
            "kNN:   8%|▊         | 3/39 [00:00<00:04,  7.66it/s, Accuracy=37.2]\u001b[A\n",
            "kNN:   8%|▊         | 3/39 [00:00<00:04,  7.66it/s, Accuracy=38.3]\u001b[A\n",
            "kNN:  10%|█         | 4/39 [00:00<00:04,  7.60it/s, Accuracy=38.3]\u001b[A\n",
            "kNN:  10%|█         | 4/39 [00:00<00:04,  7.60it/s, Accuracy=37.6]\u001b[A\n",
            "kNN:  13%|█▎        | 5/39 [00:00<00:04,  7.52it/s, Accuracy=37.6]\u001b[A\n",
            "kNN:  13%|█▎        | 5/39 [00:00<00:04,  7.52it/s, Accuracy=37.2]\u001b[A\n",
            "kNN:  15%|█▌        | 6/39 [00:00<00:04,  7.54it/s, Accuracy=37.2]\u001b[A\n",
            "kNN:  15%|█▌        | 6/39 [00:00<00:04,  7.54it/s, Accuracy=37.8]\u001b[A\n",
            "kNN:  18%|█▊        | 7/39 [00:00<00:04,  7.67it/s, Accuracy=37.8]\u001b[A\n",
            "kNN:  18%|█▊        | 7/39 [00:01<00:04,  7.67it/s, Accuracy=37.9]\u001b[A\n",
            "kNN:  21%|██        | 8/39 [00:01<00:03,  7.81it/s, Accuracy=37.9]\u001b[A\n",
            "kNN:  21%|██        | 8/39 [00:01<00:03,  7.81it/s, Accuracy=37.8]\u001b[A\n",
            "kNN:  23%|██▎       | 9/39 [00:01<00:03,  7.92it/s, Accuracy=37.8]\u001b[A\n",
            "kNN:  23%|██▎       | 9/39 [00:01<00:03,  7.92it/s, Accuracy=37.7]\u001b[A\n",
            "kNN:  26%|██▌       | 10/39 [00:01<00:03,  7.95it/s, Accuracy=37.7]\u001b[A\n",
            "kNN:  26%|██▌       | 10/39 [00:01<00:03,  7.95it/s, Accuracy=37.2]\u001b[A\n",
            "kNN:  28%|██▊       | 11/39 [00:01<00:03,  7.95it/s, Accuracy=37.2]\u001b[A\n",
            "kNN:  28%|██▊       | 11/39 [00:01<00:03,  7.95it/s, Accuracy=36.9]\u001b[A\n",
            "kNN:  31%|███       | 12/39 [00:01<00:03,  7.93it/s, Accuracy=36.9]\u001b[A\n",
            "kNN:  31%|███       | 12/39 [00:01<00:03,  7.93it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  33%|███▎      | 13/39 [00:01<00:03,  7.59it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  33%|███▎      | 13/39 [00:01<00:03,  7.59it/s, Accuracy=36.8]\u001b[A\n",
            "kNN:  36%|███▌      | 14/39 [00:01<00:03,  7.68it/s, Accuracy=36.8]\u001b[A\n",
            "kNN:  36%|███▌      | 14/39 [00:01<00:03,  7.68it/s, Accuracy=36.9]\u001b[A\n",
            "kNN:  38%|███▊      | 15/39 [00:01<00:03,  7.42it/s, Accuracy=36.9]\u001b[A\n",
            "kNN:  38%|███▊      | 15/39 [00:02<00:03,  7.42it/s, Accuracy=37]  \u001b[A\n",
            "kNN:  41%|████      | 16/39 [00:02<00:03,  7.58it/s, Accuracy=37]\u001b[A\n",
            "kNN:  41%|████      | 16/39 [00:02<00:03,  7.58it/s, Accuracy=37.3]\u001b[A\n",
            "kNN:  44%|████▎     | 17/39 [00:02<00:02,  7.73it/s, Accuracy=37.3]\u001b[A\n",
            "kNN:  44%|████▎     | 17/39 [00:02<00:02,  7.73it/s, Accuracy=37.3]\u001b[A\n",
            "kNN:  46%|████▌     | 18/39 [00:02<00:02,  7.56it/s, Accuracy=37.3]\u001b[A\n",
            "kNN:  46%|████▌     | 18/39 [00:02<00:02,  7.56it/s, Accuracy=37.4]\u001b[A\n",
            "kNN:  49%|████▊     | 19/39 [00:02<00:02,  7.46it/s, Accuracy=37.4]\u001b[A\n",
            "kNN:  49%|████▊     | 19/39 [00:02<00:02,  7.46it/s, Accuracy=37.3]\u001b[A\n",
            "kNN:  51%|█████▏    | 20/39 [00:02<00:02,  7.47it/s, Accuracy=37.3]\u001b[A\n",
            "kNN:  51%|█████▏    | 20/39 [00:02<00:02,  7.47it/s, Accuracy=37]  \u001b[A\n",
            "kNN:  54%|█████▍    | 21/39 [00:02<00:02,  7.61it/s, Accuracy=37]\u001b[A\n",
            "kNN:  54%|█████▍    | 21/39 [00:02<00:02,  7.61it/s, Accuracy=36.8]\u001b[A\n",
            "kNN:  56%|█████▋    | 22/39 [00:02<00:02,  7.73it/s, Accuracy=36.8]\u001b[A\n",
            "kNN:  56%|█████▋    | 22/39 [00:03<00:02,  7.73it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  59%|█████▉    | 23/39 [00:03<00:02,  7.66it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  59%|█████▉    | 23/39 [00:03<00:02,  7.66it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  62%|██████▏   | 24/39 [00:03<00:02,  7.39it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  62%|██████▏   | 24/39 [00:03<00:02,  7.39it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  64%|██████▍   | 25/39 [00:03<00:01,  7.50it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  64%|██████▍   | 25/39 [00:03<00:01,  7.50it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  67%|██████▋   | 26/39 [00:03<00:01,  7.53it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  67%|██████▋   | 26/39 [00:03<00:01,  7.53it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  69%|██████▉   | 27/39 [00:03<00:01,  7.72it/s, Accuracy=36.7]\u001b[A\n",
            "kNN:  69%|██████▉   | 27/39 [00:03<00:01,  7.72it/s, Accuracy=36.6]\u001b[A\n",
            "kNN:  72%|███████▏  | 28/39 [00:03<00:01,  7.79it/s, Accuracy=36.6]\u001b[A\n",
            "kNN:  72%|███████▏  | 28/39 [00:03<00:01,  7.79it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  74%|███████▍  | 29/39 [00:03<00:01,  7.72it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  74%|███████▍  | 29/39 [00:03<00:01,  7.72it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  77%|███████▋  | 30/39 [00:03<00:01,  7.79it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  77%|███████▋  | 30/39 [00:04<00:01,  7.79it/s, Accuracy=36.3]\u001b[A\n",
            "kNN:  79%|███████▉  | 31/39 [00:04<00:01,  7.77it/s, Accuracy=36.3]\u001b[A\n",
            "kNN:  79%|███████▉  | 31/39 [00:04<00:01,  7.77it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  82%|████████▏ | 32/39 [00:04<00:00,  7.89it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  82%|████████▏ | 32/39 [00:04<00:00,  7.89it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  85%|████████▍ | 33/39 [00:04<00:00,  7.99it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  85%|████████▍ | 33/39 [00:04<00:00,  7.99it/s, Accuracy=36.3]\u001b[A\n",
            "kNN:  87%|████████▋ | 34/39 [00:04<00:00,  7.82it/s, Accuracy=36.3]\u001b[A\n",
            "kNN:  87%|████████▋ | 34/39 [00:04<00:00,  7.82it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  90%|████████▉ | 35/39 [00:04<00:00,  7.92it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  90%|████████▉ | 35/39 [00:04<00:00,  7.92it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  92%|█████████▏| 36/39 [00:04<00:00,  7.87it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  92%|█████████▏| 36/39 [00:04<00:00,  7.87it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  95%|█████████▍| 37/39 [00:04<00:00,  7.95it/s, Accuracy=36.4]\u001b[A\n",
            "kNN:  95%|█████████▍| 37/39 [00:04<00:00,  7.95it/s, Accuracy=36.6]\u001b[A\n",
            "kNN:  97%|█████████▋| 38/39 [00:04<00:00,  7.84it/s, Accuracy=36.6]\u001b[A\n",
            "kNN:  97%|█████████▋| 38/39 [00:05<00:00,  7.84it/s, Accuracy=36.4]\u001b[A\n",
            "kNN: 100%|██████████| 39/39 [00:05<00:00,  7.71it/s, Accuracy=36.4]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 36.44831730769231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQRoZnDv3JOR"
      },
      "source": [
        "class AverageMeter():\r\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\r\n",
        "    def __init__(self, name, fmt=':f'):\r\n",
        "\r\n",
        "      \r\n",
        "        self.name = name\r\n",
        "        self.fmt = fmt\r\n",
        "        self.log = []\r\n",
        "        self.val = 0\r\n",
        "        self.avg = 0\r\n",
        "        self.sum = 0\r\n",
        "        self.count = 0\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.log.append(self.avg)\r\n",
        "        self.val = 0\r\n",
        "        self.avg = 0\r\n",
        "        self.sum = 0\r\n",
        "        self.count = 0\r\n",
        "\r\n",
        "    def update(self, val, n=1):\r\n",
        "        self.val = val\r\n",
        "        self.sum += val * n\r\n",
        "        self.count += n\r\n",
        "        self.avg = self.sum / self.count\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\r\n",
        "        return fmtstr.format(**self.__dict__)\r\n",
        "\r\n",
        "class LR_Scheduler(object):\r\n",
        "  def __init__(self, optimizer, warmup_epochs, warmup_lr, num_epochs, base_lr, final_lr, iter_per_epoch, constant_predictor_lr=False):\r\n",
        "    self.base_lr = base_lr\r\n",
        "    self.constant_predictor_lr = constant_predictor_lr\r\n",
        "    warmup_iter = iter_per_epoch * warmup_epochs\r\n",
        "    warmup_lr_schedule = np.linspace(warmup_lr, base_lr, warmup_iter)\r\n",
        "    decay_iter = iter_per_epoch * (num_epochs - warmup_epochs)\r\n",
        "    cosine_lr_schedule = final_lr+0.5*(base_lr-final_lr)*(1+np.cos(np.pi*np.arange(decay_iter)/decay_iter))\r\n",
        "    \r\n",
        "    self.lr_schedule = np.concatenate((warmup_lr_schedule, cosine_lr_schedule))\r\n",
        "    self.optimizer = optimizer\r\n",
        "    self.iter = 0\r\n",
        "    self.current_lr = 0\r\n",
        "  def step(self):\r\n",
        "    for param_group in self.optimizer.param_groups:\r\n",
        "\r\n",
        "      if self.constant_predictor_lr and param_group['name'] == 'predictor':\r\n",
        "        param_group['lr'] = self.base_lr\r\n",
        "      else:\r\n",
        "        lr = param_group['lr'] = self.lr_schedule[self.iter]\r\n",
        "    \r\n",
        "    self.iter += 1\r\n",
        "    self.current_lr = lr\r\n",
        "    return lr\r\n",
        "  def get_lr(self):\r\n",
        "    return self.current_lr\r\n",
        "\r\n",
        "def linear_eval(args, eval_from):\r\n",
        "  eval_train = torchvision.datasets.CIFAR10(\r\n",
        "      root=args.dataset_dir, \r\n",
        "      train=True, \r\n",
        "      download=False, \r\n",
        "      transform=Transform_single(size=args.image_size, train=True), \r\n",
        "      )\r\n",
        "  eval_train_loader = torch.utils.data.DataLoader(\r\n",
        "      eval_train, \r\n",
        "      shuffle=True,\r\n",
        "      batch_size=args.batch_size,\r\n",
        "      num_workers=args.num_workers,\r\n",
        "      drop_last=True,\r\n",
        "      pin_memory=True,\r\n",
        "      )\r\n",
        "\r\n",
        "  eval_test = torchvision.datasets.CIFAR10(\r\n",
        "      root=args.dataset_dir, \r\n",
        "      train=False, \r\n",
        "      download=False, \r\n",
        "      transform=Transform_single(size=args.image_size, train=False), \r\n",
        "      )\r\n",
        "  eval_test_loader = torch.utils.data.DataLoader(\r\n",
        "      eval_test, \r\n",
        "      shuffle=False,\r\n",
        "      batch_size=args.batch_size,\r\n",
        "      num_workers=args.num_workers,\r\n",
        "      drop_last=True,\r\n",
        "      pin_memory=True,\r\n",
        "      )\r\n",
        "    \r\n",
        "  eval_model = eval(f\"models.{args.resnet_version}()\")\r\n",
        "  eval_model.output_dim = eval_model.fc.in_features\r\n",
        "  eval_model.fc = torch.nn.Identity()\r\n",
        "  eval_classifier = nn.Linear(in_features=eval_model.output_dim, out_features=10, bias=True).to(args.device)\r\n",
        "\r\n",
        "  ###\r\n",
        "  assert eval_from is not None\r\n",
        "  eval_save_dict = torch.load(eval_from, map_location='cuda')\r\n",
        "  # eval_msg = eval_model.load_state_dict({k[9:]:v for k, v in eval_save_dict['state_dict'].items() if k.startswith('backbone.')}, strict=True)\r\n",
        "  \r\n",
        "  # print(eval_msg)\r\n",
        "  eval_model = eval_model.to(args.device)\r\n",
        "  eval_model = torch.nn.DataParallel(eval_model)\r\n",
        "  # if torch.cuda.device_count() > 1: eval_classifier = torch.nn.SyncBatchNorm.convert_sync_batchnorm(eval_classifier)\r\n",
        "  eval_classifier = torch.nn.DataParallel(eval_classifier)\r\n",
        "  # define optimizer 'sgd', eval_classifier, lr=eval_base_lr=30, momentum=eval_optim_momentum-0.9, weight_decay=eval_optim_weight_decay=0\r\n",
        "  predictor_prefix = ('module.predictor', 'predictor')\r\n",
        "  parameters = [{\r\n",
        "      'name': 'base',\r\n",
        "      'params': [param for name, param in eval_classifier.named_parameters() if not name.startswith(predictor_prefix)],\r\n",
        "      'lr': 30\r\n",
        "  },{\r\n",
        "      'name': 'predictor',\r\n",
        "      'params': [param for name, param in eval_classifier.named_parameters() if name.startswith(predictor_prefix)],\r\n",
        "      'lr': 30\r\n",
        "  }]\r\n",
        "  eval_optimizer = torch.optim.SGD(parameters, lr=30, momentum=0.9, weight_decay=0)\r\n",
        "\r\n",
        "  # define lr scheduler\r\n",
        "  eval_lr_scheduler = LR_Scheduler(\r\n",
        "      eval_optimizer,\r\n",
        "      0, 0*args.batch_size/256, \r\n",
        "      30, 30*args.batch_size/256, 0*args.batch_size/256, \r\n",
        "      len(eval_train_loader),\r\n",
        "  )\r\n",
        "\r\n",
        "  eval_loss_meter = AverageMeter(name='Loss')\r\n",
        "  eval_acc_meter = AverageMeter(name='Accuracy')\r\n",
        "\r\n",
        "  # Start training\r\n",
        "  eval_global_progress = tqdm(range(0, 30), desc=f'Evaluating')\r\n",
        "  for epoch in eval_global_progress:\r\n",
        "    eval_loss_meter.reset()\r\n",
        "    eval_model.eval()\r\n",
        "    eval_classifier.train()\r\n",
        "    eval_local_progress = tqdm(eval_train_loader, desc=f'Epoch {epoch}/{30}', disable=True)\r\n",
        "    \r\n",
        "    for idx, (images, labels) in enumerate(eval_local_progress):\r\n",
        "\r\n",
        "      eval_classifier.zero_grad()\r\n",
        "      with torch.no_grad():\r\n",
        "        eval_feature = eval_model(images.to(args.device))\r\n",
        "\r\n",
        "      eval_preds = eval_classifier(eval_feature)\r\n",
        "\r\n",
        "      eval_loss = F.cross_entropy(eval_preds, labels.to(args.device))\r\n",
        "\r\n",
        "      eval_loss.backward()\r\n",
        "      eval_optimizer.step()\r\n",
        "      eval_loss_meter.update(eval_loss.item())\r\n",
        "      eval_lr = eval_lr_scheduler.step()\r\n",
        "      eval_local_progress.set_postfix({'lr':eval_lr, \"loss\":eval_loss_meter.val, 'loss_avg':eval_loss_meter.avg})\r\n",
        "\r\n",
        "  eval_classifier.eval()\r\n",
        "  eval_correct, eval_total = 0, 0\r\n",
        "  eval_acc_meter.reset()\r\n",
        "  for idx, (images, labels) in enumerate(eval_test_loader):\r\n",
        "    with torch.no_grad():\r\n",
        "      eval_feature = eval_model(images.to(args.device))\r\n",
        "      eval_preds = eval_classifier(eval_feature).argmax(dim=1)\r\n",
        "      eval_correct = (eval_preds == labels.to(args.device)).sum().item()\r\n",
        "      eval_acc_meter.update(eval_correct/eval_preds.shape[0])\r\n",
        "  print(f'Accuracy = {eval_acc_meter.avg*100:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ik3Lsil3Mfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd5ffab-bf50-4784-f5d5-be3b4e0a4aaf"
      },
      "source": [
        "if args.eval is not False:\r\n",
        "  linear_eval(args, ckpt_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 1/30 [00:25<12:11, 25.24s/it]\u001b[A\n",
            "Evaluating:   7%|▋         | 2/30 [00:50<11:49, 25.33s/it]\u001b[A\n",
            "Evaluating:  10%|█         | 3/30 [01:16<11:25, 25.38s/it]\u001b[A\n",
            "Evaluating:  13%|█▎        | 4/30 [01:41<10:58, 25.33s/it]\u001b[A\n",
            "Evaluating:  17%|█▋        | 5/30 [02:06<10:29, 25.20s/it]\u001b[A\n",
            "Evaluating:  20%|██        | 6/30 [02:32<10:09, 25.40s/it]\u001b[A\n",
            "Evaluating:  23%|██▎       | 7/30 [02:58<09:47, 25.56s/it]\u001b[A\n",
            "Evaluating:  27%|██▋       | 8/30 [03:23<09:23, 25.62s/it]\u001b[A\n",
            "Evaluating:  30%|███       | 9/30 [03:50<09:01, 25.76s/it]\u001b[A\n",
            "Evaluating:  33%|███▎      | 10/30 [04:16<08:37, 25.87s/it]\u001b[A\n",
            "Evaluating:  37%|███▋      | 11/30 [04:42<08:13, 25.96s/it]\u001b[A\n",
            "Evaluating:  40%|████      | 12/30 [05:08<07:47, 25.99s/it]\u001b[A\n",
            "Evaluating:  43%|████▎     | 13/30 [05:34<07:23, 26.11s/it]\u001b[A\n",
            "Evaluating:  47%|████▋     | 14/30 [06:02<07:04, 26.53s/it]\u001b[A\n",
            "Evaluating:  50%|█████     | 15/30 [06:28<06:35, 26.38s/it]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 16/30 [06:54<06:06, 26.17s/it]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 17/30 [07:20<05:40, 26.20s/it]\u001b[A\n",
            "Evaluating:  60%|██████    | 18/30 [07:46<05:12, 26.07s/it]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 19/30 [08:11<04:45, 25.93s/it]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 20/30 [08:37<04:19, 25.90s/it]\u001b[A\n",
            "Evaluating:  70%|███████   | 21/30 [09:02<03:51, 25.75s/it]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 22/30 [09:28<03:26, 25.81s/it]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 23/30 [09:55<03:01, 25.91s/it]\u001b[A\n",
            "Evaluating:  80%|████████  | 24/30 [10:19<02:33, 25.64s/it]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 25/30 [10:45<02:07, 25.49s/it]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 26/30 [11:11<01:42, 25.73s/it]\u001b[A\n",
            "Evaluating:  90%|█████████ | 27/30 [11:37<01:17, 25.78s/it]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 28/30 [12:02<00:51, 25.68s/it]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 29/30 [12:28<00:25, 25.59s/it]\u001b[A\n",
            "Evaluating: 100%|██████████| 30/30 [12:53<00:00, 25.77s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 34.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V6nzE-dPRpT"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "from torchvision import models\r\n",
        "\r\n",
        "if args.resnet_version is not None:\r\n",
        "  resnet2 = eval(f'models.{args.resnet_version}()')\r\n",
        "  # resnet = eval(f\"{backbone_name}()\")\r\n",
        "  resnet2.output_dim = resnet2.fc.in_features\r\n",
        "  resnet2.fc = nn.Identity()\r\n",
        "else:\r\n",
        "  raise NotImplementedError(\"Backbone is not implemented!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkg9hmeoYsPm"
      },
      "source": [
        "import copy\r\n",
        "import math\r\n",
        "from torch.nn import functional\r\n",
        "\r\n",
        "class MLP2(nn.Module):\r\n",
        "  def __init__(self, input_dim):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.net = nn.Sequential(\r\n",
        "        nn.Linear(input_dim, 4096), \r\n",
        "        nn.BatchNorm1d(4096, momentum=1-0.9, eps=1e-5), \r\n",
        "        nn.ReLU(inplace=True), \r\n",
        "        nn.Linear(4096, 256)\r\n",
        "    )\r\n",
        "  def forward(self, x):\r\n",
        "    return self.net(x)\r\n",
        "\r\n",
        "class BYOL2(nn.Module):\r\n",
        "  def __init__(self, backbone):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.backbone=backbone\r\n",
        "    self.projector = MLP2(resnet2.output_dim)\r\n",
        "    self.online_encoder = nn.Sequential(\r\n",
        "        self.backbone, \r\n",
        "        self.projector,\r\n",
        "    )\r\n",
        "    self.predictor = MLP2(256)\r\n",
        "    self.target_encoder = copy.deepcopy(self.online_encoder)\r\n",
        "\r\n",
        "  def target_ema(self, k, K, base_tau=0.996):\r\n",
        "    return 1-(1-base_tau)*(math.cos(math.pi*k/K)+1)/2\r\n",
        "\r\n",
        "  def reset_moving_average(self):\r\n",
        "    del self.target_encoder\r\n",
        "    self.target_encoder = copy.deepcopy(self.online_encoder)\r\n",
        "\r\n",
        "  def update_moving_average(self, global_step, max_steps):\r\n",
        "    tau = self.target_ema(global_step, max_steps)\r\n",
        "    for online, target in zip(self.online_encoder.parameters(), self.target_encoder.parameters()):\r\n",
        "      target.data = tau*target.data + (1-tau)*online.data\r\n",
        "  \r\n",
        "  def loss_function(self, p, z):\r\n",
        "    p=functional.normalize(p, dim=-1, p=2)\r\n",
        "    z=functional.normalize(z, dim=-1, p=2)\r\n",
        "    return 2 - 2*(p*z).sum(dim=-1)\r\n",
        "\r\n",
        "  def forward(self, x1, x2):\r\n",
        "    z1_online, z2_online = self.online_encoder(x1), self.online_encoder(x2)\r\n",
        "    p1_online, p2_online = self.predictor(z1_online), self.predictor(z2_online)\r\n",
        "    with torch.no_grad():\r\n",
        "      z1_target, z2_target = self.target_encoder(x1), self.target_encoder(x2)\r\n",
        "    \r\n",
        "    loss1, loss2 = self.loss_function(p1_online, z2_target.detach()), self.loss_function(p2_online, z1_target.detach())\r\n",
        "\r\n",
        "    loss = loss1+loss2\r\n",
        "    return loss.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9etMFJRPljr"
      },
      "source": [
        "byol = BYOL2(resnet2)\r\n",
        "byol = byol.to(args.device)\r\n",
        "byol = torch.nn.DataParallel(byol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rMlyFb-PusT"
      },
      "source": [
        "predictor_prefix2 = ('module.predictor', 'predictor')\r\n",
        "parameters2 = [{\r\n",
        "    'name': 'base',\r\n",
        "    'params': [param for name, param in byol.named_parameters() if not name.startswith(predictor_prefix2)],\r\n",
        "    'lr': args.learning_rate\r\n",
        "},{\r\n",
        "    'name': 'predictor',\r\n",
        "    'params': [param for name, param in byol.named_parameters() if name.startswith(predictor_prefix2)],\r\n",
        "    'lr': args.learning_rate\r\n",
        "}]\r\n",
        "if args.optim == 'lars':\r\n",
        "  optimizer2 = LARS(byol.named_modules(), lr=args.learning_rate*args.batch_size/256, momentum=args.momentum, weight_decay=args.weight_decay)\r\n",
        "elif args.optim == 'adam':\r\n",
        "  optimizer2 = Adam(parameters2, lr=args.learning_rate*args.batch_size/256)\r\n",
        "elif args.optim == 'sgd':\r\n",
        "  optimizer2 = SGD(parameters2, lr=args.learning_rate*args.batch_size/256, momentum=0.9)\r\n",
        "else: # default is LARS\r\n",
        "  optimizer2 = LARS(byol.named_modules(), lr=args.learning_rate*args.batch_size/256, weight_decay=args.weight_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VUWMbYrQiFr",
        "outputId": "01d218d2-45d2-40b0-97f4-254d6fb0880c"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "from collections import defaultdict\r\n",
        "from datetime import datetime\r\n",
        "import os\r\n",
        "\r\n",
        "# if not os.path.exists('./ckpt'):\r\n",
        "#   os.makedirs('./ckpt')\r\n",
        "\r\n",
        "# tmp_dir = os.path.join('./ckpt', f\"{args.resnet_version}\")\r\n",
        "# if not os.path.exists(tmp_dir):\r\n",
        "#   os.makedirs(tmp_dir)\r\n",
        "# tmp_dir = os.path.join(tmp_dir, f\"{args.optim}\")\r\n",
        "# if not os.path.exists(tmp_dir):\r\n",
        "#   os.makedirs(tmp_dir)\r\n",
        "# tmp_dir = os.path.join(tmp_dir, f\"{datetime.now().strftime('%m%d%H')}\")\r\n",
        "# if not os.path.exists(tmp_dir):\r\n",
        "#   os.makedirs(tmp_dir)\r\n",
        "\r\n",
        "writer2 = SummaryWriter()\r\n",
        "\r\n",
        "global_step = 0\r\n",
        "for epoch in tqdm(range(0, args.num_epochs)):\r\n",
        "  metrics = defaultdict(list)\r\n",
        "  \r\n",
        "  for step, ((x1, x2), labels) in enumerate(train_loader):\r\n",
        "    x1, x2 = x1.cuda(non_blocking=True), x2.cuda(non_blocking=True)\r\n",
        "\r\n",
        "    main_loss = byol(x1, x2)\r\n",
        "    optimizer2.zero_grad()\r\n",
        "    main_loss.backward()\r\n",
        "    optimizer2.step()\r\n",
        "    byol.module.update_moving_average(epoch, args.num_epochs)\r\n",
        "    \r\n",
        "    writer2.add_scalar(\"Loss/train_step\", main_loss, global_step)\r\n",
        "    metrics[\"Loss/train\"].append(main_loss.item())\r\n",
        "    global_step += 1\r\n",
        "  \r\n",
        "  for k, v in metrics.items():\r\n",
        "    writer2.add_scalar(k, np.array(v).mean(), epoch)\r\n",
        "\r\n",
        "  if epoch%args.checkpoint_epochs == 0:\r\n",
        "    ckpt_path = os.path.join(tmp_dir, f\"byol2_{args.optim}_{epoch}.pt\")\r\n",
        "    print(f'Saving model at epoch {epoch}')\r\n",
        "    torch.save(resnet2.state_dict(), ckpt_path)\r\n",
        "\r\n",
        "ckpt_path = os.path.join(tmp_dir, f\"byol2_{args.optim}_final.pt\")\r\n",
        "print('Saving final model')\r\n",
        "torch.save(resnet2.state_dict(), ckpt_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [02:23<21:27, 143.01s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving model at epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 20%|██        | 2/10 [04:44<19:00, 142.56s/it]\u001b[A\n",
            " 30%|███       | 3/10 [07:03<16:31, 141.61s/it]\u001b[A\n",
            " 40%|████      | 4/10 [09:24<14:08, 141.40s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [11:45<11:46, 141.31s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [14:06<09:24, 141.09s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [16:26<07:02, 140.73s/it]\u001b[A\n",
            " 80%|████████  | 8/10 [18:48<04:42, 141.06s/it]\u001b[A\n",
            " 90%|█████████ | 9/10 [21:11<02:21, 141.79s/it]\u001b[A\n",
            "100%|██████████| 10/10 [23:33<00:00, 141.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving final model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWq7ay-PSim6",
        "outputId": "29e78995-6518-45c2-bae4-a48e5010d0c6"
      },
      "source": [
        "accuracy2 = knn_monitor(byol.module.backbone, memory_loader, test_loader, k=min(200, len(memory_loader.dataset)), hide_progress=True)\r\n",
        "print('Accuracy:', accuracy2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Feature extracting:   0%|          | 0/195 [00:00<?, ?it/s]\u001b[A\n",
            "Feature extracting:   1%|          | 2/195 [00:00<00:16, 11.91it/s]\u001b[A\n",
            "Feature extracting:   2%|▏         | 4/195 [00:00<00:15, 11.97it/s]\u001b[A\n",
            "Feature extracting:   3%|▎         | 6/195 [00:00<00:15, 12.15it/s]\u001b[A\n",
            "Feature extracting:   4%|▍         | 8/195 [00:00<00:15, 11.78it/s]\u001b[A\n",
            "Feature extracting:   5%|▌         | 10/195 [00:00<00:15, 11.61it/s]\u001b[A\n",
            "Feature extracting:   6%|▌         | 12/195 [00:01<00:16, 11.36it/s]\u001b[A\n",
            "Feature extracting:   7%|▋         | 13/195 [00:01<00:17, 10.64it/s]\u001b[A\n",
            "Feature extracting:   7%|▋         | 14/195 [00:01<00:17, 10.32it/s]\u001b[A\n",
            "Feature extracting:   8%|▊         | 16/195 [00:01<00:16, 10.55it/s]\u001b[A\n",
            "Feature extracting:   9%|▉         | 18/195 [00:01<00:16, 10.77it/s]\u001b[A\n",
            "Feature extracting:  10%|█         | 20/195 [00:01<00:15, 11.26it/s]\u001b[A\n",
            "Feature extracting:  11%|█▏        | 22/195 [00:01<00:14, 11.69it/s]\u001b[A\n",
            "Feature extracting:  12%|█▏        | 24/195 [00:02<00:14, 12.08it/s]\u001b[A\n",
            "Feature extracting:  13%|█▎        | 26/195 [00:02<00:13, 12.36it/s]\u001b[A\n",
            "Feature extracting:  14%|█▍        | 28/195 [00:02<00:13, 12.45it/s]\u001b[A\n",
            "Feature extracting:  15%|█▌        | 30/195 [00:02<00:13, 12.08it/s]\u001b[A\n",
            "Feature extracting:  16%|█▋        | 32/195 [00:02<00:13, 12.00it/s]\u001b[A\n",
            "Feature extracting:  17%|█▋        | 34/195 [00:02<00:13, 12.35it/s]\u001b[A\n",
            "Feature extracting:  18%|█▊        | 36/195 [00:03<00:12, 12.63it/s]\u001b[A\n",
            "Feature extracting:  19%|█▉        | 38/195 [00:03<00:12, 12.79it/s]\u001b[A\n",
            "Feature extracting:  21%|██        | 40/195 [00:03<00:12, 12.62it/s]\u001b[A\n",
            "Feature extracting:  22%|██▏       | 42/195 [00:03<00:12, 12.66it/s]\u001b[A\n",
            "Feature extracting:  23%|██▎       | 44/195 [00:03<00:12, 12.34it/s]\u001b[A\n",
            "Feature extracting:  24%|██▎       | 46/195 [00:03<00:12, 12.07it/s]\u001b[A\n",
            "Feature extracting:  25%|██▍       | 48/195 [00:04<00:12, 12.13it/s]\u001b[A\n",
            "Feature extracting:  26%|██▌       | 50/195 [00:04<00:11, 12.40it/s]\u001b[A\n",
            "Feature extracting:  27%|██▋       | 52/195 [00:04<00:11, 12.59it/s]\u001b[A\n",
            "Feature extracting:  28%|██▊       | 54/195 [00:04<00:11, 12.30it/s]\u001b[A\n",
            "Feature extracting:  29%|██▊       | 56/195 [00:04<00:11, 12.50it/s]\u001b[A\n",
            "Feature extracting:  30%|██▉       | 58/195 [00:04<00:10, 12.74it/s]\u001b[A\n",
            "Feature extracting:  31%|███       | 60/195 [00:04<00:10, 12.90it/s]\u001b[A\n",
            "Feature extracting:  32%|███▏      | 62/195 [00:05<00:10, 13.11it/s]\u001b[A\n",
            "Feature extracting:  33%|███▎      | 64/195 [00:05<00:10, 13.07it/s]\u001b[A\n",
            "Feature extracting:  34%|███▍      | 66/195 [00:05<00:10, 12.54it/s]\u001b[A\n",
            "Feature extracting:  35%|███▍      | 68/195 [00:05<00:10, 12.69it/s]\u001b[A\n",
            "Feature extracting:  36%|███▌      | 70/195 [00:05<00:10, 12.45it/s]\u001b[A\n",
            "Feature extracting:  37%|███▋      | 72/195 [00:05<00:09, 12.65it/s]\u001b[A\n",
            "Feature extracting:  38%|███▊      | 74/195 [00:06<00:09, 12.87it/s]\u001b[A\n",
            "Feature extracting:  39%|███▉      | 76/195 [00:06<00:09, 12.91it/s]\u001b[A\n",
            "Feature extracting:  40%|████      | 78/195 [00:06<00:09, 12.79it/s]\u001b[A\n",
            "Feature extracting:  41%|████      | 80/195 [00:06<00:08, 12.81it/s]\u001b[A\n",
            "Feature extracting:  42%|████▏     | 82/195 [00:06<00:08, 13.05it/s]\u001b[A\n",
            "Feature extracting:  43%|████▎     | 84/195 [00:06<00:08, 13.10it/s]\u001b[A\n",
            "Feature extracting:  44%|████▍     | 86/195 [00:06<00:08, 13.28it/s]\u001b[A\n",
            "Feature extracting:  45%|████▌     | 88/195 [00:07<00:08, 13.31it/s]\u001b[A\n",
            "Feature extracting:  46%|████▌     | 90/195 [00:07<00:07, 13.44it/s]\u001b[A\n",
            "Feature extracting:  47%|████▋     | 92/195 [00:07<00:08, 12.58it/s]\u001b[A\n",
            "Feature extracting:  48%|████▊     | 94/195 [00:07<00:08, 12.60it/s]\u001b[A\n",
            "Feature extracting:  49%|████▉     | 96/195 [00:07<00:08, 12.30it/s]\u001b[A\n",
            "Feature extracting:  50%|█████     | 98/195 [00:07<00:08, 12.10it/s]\u001b[A\n",
            "Feature extracting:  51%|█████▏    | 100/195 [00:08<00:08, 11.84it/s]\u001b[A\n",
            "Feature extracting:  52%|█████▏    | 102/195 [00:08<00:08, 11.41it/s]\u001b[A\n",
            "Feature extracting:  53%|█████▎    | 104/195 [00:08<00:08, 10.68it/s]\u001b[A\n",
            "Feature extracting:  54%|█████▍    | 106/195 [00:08<00:08, 11.11it/s]\u001b[A\n",
            "Feature extracting:  55%|█████▌    | 108/195 [00:08<00:07, 11.55it/s]\u001b[A\n",
            "Feature extracting:  56%|█████▋    | 110/195 [00:08<00:07, 11.77it/s]\u001b[A\n",
            "Feature extracting:  57%|█████▋    | 112/195 [00:09<00:06, 11.98it/s]\u001b[A\n",
            "Feature extracting:  58%|█████▊    | 114/195 [00:09<00:06, 12.10it/s]\u001b[A\n",
            "Feature extracting:  59%|█████▉    | 116/195 [00:09<00:06, 11.86it/s]\u001b[A\n",
            "Feature extracting:  61%|██████    | 118/195 [00:09<00:06, 11.71it/s]\u001b[A\n",
            "Feature extracting:  62%|██████▏   | 120/195 [00:09<00:06, 11.65it/s]\u001b[A\n",
            "Feature extracting:  63%|██████▎   | 122/195 [00:10<00:06, 11.74it/s]\u001b[A\n",
            "Feature extracting:  64%|██████▎   | 124/195 [00:10<00:05, 12.12it/s]\u001b[A\n",
            "Feature extracting:  65%|██████▍   | 126/195 [00:10<00:05, 12.32it/s]\u001b[A\n",
            "Feature extracting:  66%|██████▌   | 128/195 [00:10<00:05, 11.90it/s]\u001b[A\n",
            "Feature extracting:  67%|██████▋   | 130/195 [00:10<00:05, 11.90it/s]\u001b[A\n",
            "Feature extracting:  68%|██████▊   | 132/195 [00:10<00:05, 12.26it/s]\u001b[A\n",
            "Feature extracting:  69%|██████▊   | 134/195 [00:10<00:04, 12.59it/s]\u001b[A\n",
            "Feature extracting:  70%|██████▉   | 136/195 [00:11<00:04, 12.94it/s]\u001b[A\n",
            "Feature extracting:  71%|███████   | 138/195 [00:11<00:04, 12.98it/s]\u001b[A\n",
            "Feature extracting:  72%|███████▏  | 140/195 [00:11<00:04, 13.19it/s]\u001b[A\n",
            "Feature extracting:  73%|███████▎  | 142/195 [00:11<00:04, 12.95it/s]\u001b[A\n",
            "Feature extracting:  74%|███████▍  | 144/195 [00:11<00:03, 13.03it/s]\u001b[A\n",
            "Feature extracting:  75%|███████▍  | 146/195 [00:11<00:03, 13.30it/s]\u001b[A\n",
            "Feature extracting:  76%|███████▌  | 148/195 [00:12<00:03, 13.36it/s]\u001b[A\n",
            "Feature extracting:  77%|███████▋  | 150/195 [00:12<00:03, 13.20it/s]\u001b[A\n",
            "Feature extracting:  78%|███████▊  | 152/195 [00:12<00:03, 12.95it/s]\u001b[A\n",
            "Feature extracting:  79%|███████▉  | 154/195 [00:12<00:03, 12.80it/s]\u001b[A\n",
            "Feature extracting:  80%|████████  | 156/195 [00:12<00:03, 12.80it/s]\u001b[A\n",
            "Feature extracting:  81%|████████  | 158/195 [00:12<00:02, 13.07it/s]\u001b[A\n",
            "Feature extracting:  82%|████████▏ | 160/195 [00:12<00:02, 13.11it/s]\u001b[A\n",
            "Feature extracting:  83%|████████▎ | 162/195 [00:13<00:02, 13.20it/s]\u001b[A\n",
            "Feature extracting:  84%|████████▍ | 164/195 [00:13<00:02, 13.35it/s]\u001b[A\n",
            "Feature extracting:  85%|████████▌ | 166/195 [00:13<00:02, 13.12it/s]\u001b[A\n",
            "Feature extracting:  86%|████████▌ | 168/195 [00:13<00:02, 13.00it/s]\u001b[A\n",
            "Feature extracting:  87%|████████▋ | 170/195 [00:13<00:01, 12.82it/s]\u001b[A\n",
            "Feature extracting:  88%|████████▊ | 172/195 [00:13<00:01, 12.90it/s]\u001b[A\n",
            "Feature extracting:  89%|████████▉ | 174/195 [00:14<00:01, 12.59it/s]\u001b[A\n",
            "Feature extracting:  90%|█████████ | 176/195 [00:14<00:01, 12.89it/s]\u001b[A\n",
            "Feature extracting:  91%|█████████▏| 178/195 [00:14<00:01, 12.39it/s]\u001b[A\n",
            "Feature extracting:  92%|█████████▏| 180/195 [00:14<00:01, 12.20it/s]\u001b[A\n",
            "Feature extracting:  93%|█████████▎| 182/195 [00:14<00:01, 12.27it/s]\u001b[A\n",
            "Feature extracting:  94%|█████████▍| 184/195 [00:14<00:00, 12.47it/s]\u001b[A\n",
            "Feature extracting:  95%|█████████▌| 186/195 [00:15<00:00, 12.47it/s]\u001b[A\n",
            "Feature extracting:  96%|█████████▋| 188/195 [00:15<00:00, 12.80it/s]\u001b[A\n",
            "Feature extracting:  97%|█████████▋| 190/195 [00:15<00:00, 12.82it/s]\u001b[A\n",
            "Feature extracting:  98%|█████████▊| 192/195 [00:15<00:00, 13.03it/s]\u001b[A\n",
            "Feature extracting:  99%|█████████▉| 194/195 [00:15<00:00, 12.31it/s]\u001b[A\n",
            "                                                                     \u001b[A\n",
            "kNN:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "kNN:   0%|          | 0/39 [00:00<?, ?it/s, Accuracy=35.5]\u001b[A\n",
            "kNN:   3%|▎         | 1/39 [00:00<00:04,  8.21it/s, Accuracy=35.5]\u001b[A\n",
            "kNN:   3%|▎         | 1/39 [00:00<00:04,  8.21it/s, Accuracy=35.9]\u001b[A\n",
            "kNN:   5%|▌         | 2/39 [00:00<00:04,  7.99it/s, Accuracy=35.9]\u001b[A\n",
            "kNN:   5%|▌         | 2/39 [00:00<00:04,  7.99it/s, Accuracy=35.7]\u001b[A\n",
            "kNN:   8%|▊         | 3/39 [00:00<00:04,  7.91it/s, Accuracy=35.7]\u001b[A\n",
            "kNN:   8%|▊         | 3/39 [00:00<00:04,  7.91it/s, Accuracy=36.1]\u001b[A\n",
            "kNN:  10%|█         | 4/39 [00:00<00:04,  7.86it/s, Accuracy=36.1]\u001b[A\n",
            "kNN:  10%|█         | 4/39 [00:00<00:04,  7.86it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  13%|█▎        | 5/39 [00:00<00:04,  7.54it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  13%|█▎        | 5/39 [00:00<00:04,  7.54it/s, Accuracy=35]  \u001b[A\n",
            "kNN:  15%|█▌        | 6/39 [00:00<00:04,  7.46it/s, Accuracy=35]\u001b[A\n",
            "kNN:  15%|█▌        | 6/39 [00:00<00:04,  7.46it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  18%|█▊        | 7/39 [00:00<00:04,  7.71it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  18%|█▊        | 7/39 [00:01<00:04,  7.71it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  21%|██        | 8/39 [00:01<00:03,  7.86it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  21%|██        | 8/39 [00:01<00:03,  7.86it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  23%|██▎       | 9/39 [00:01<00:03,  7.98it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  23%|██▎       | 9/39 [00:01<00:03,  7.98it/s, Accuracy=35.6]\u001b[A\n",
            "kNN:  26%|██▌       | 10/39 [00:01<00:03,  8.17it/s, Accuracy=35.6]\u001b[A\n",
            "kNN:  26%|██▌       | 10/39 [00:01<00:03,  8.17it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  28%|██▊       | 11/39 [00:01<00:03,  8.06it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  28%|██▊       | 11/39 [00:01<00:03,  8.06it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  31%|███       | 12/39 [00:01<00:03,  8.16it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  31%|███       | 12/39 [00:01<00:03,  8.16it/s, Accuracy=35.2]\u001b[A\n",
            "kNN:  33%|███▎      | 13/39 [00:01<00:03,  8.20it/s, Accuracy=35.2]\u001b[A\n",
            "kNN:  33%|███▎      | 13/39 [00:01<00:03,  8.20it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  36%|███▌      | 14/39 [00:01<00:03,  8.23it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  36%|███▌      | 14/39 [00:01<00:03,  8.23it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  38%|███▊      | 15/39 [00:01<00:02,  8.05it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  38%|███▊      | 15/39 [00:02<00:02,  8.05it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  41%|████      | 16/39 [00:02<00:02,  7.87it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  41%|████      | 16/39 [00:02<00:02,  7.87it/s, Accuracy=35.5]\u001b[A\n",
            "kNN:  44%|████▎     | 17/39 [00:02<00:02,  7.50it/s, Accuracy=35.5]\u001b[A\n",
            "kNN:  44%|████▎     | 17/39 [00:02<00:02,  7.50it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  46%|████▌     | 18/39 [00:02<00:02,  7.53it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  46%|████▌     | 18/39 [00:02<00:02,  7.53it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  49%|████▊     | 19/39 [00:02<00:02,  7.42it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  49%|████▊     | 19/39 [00:02<00:02,  7.42it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  51%|█████▏    | 20/39 [00:02<00:02,  7.52it/s, Accuracy=35.4]\u001b[A\n",
            "kNN:  51%|█████▏    | 20/39 [00:02<00:02,  7.52it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  54%|█████▍    | 21/39 [00:02<00:02,  7.51it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  54%|█████▍    | 21/39 [00:02<00:02,  7.51it/s, Accuracy=35.1]\u001b[A\n",
            "kNN:  56%|█████▋    | 22/39 [00:02<00:02,  7.30it/s, Accuracy=35.1]\u001b[A\n",
            "kNN:  56%|█████▋    | 22/39 [00:02<00:02,  7.30it/s, Accuracy=35]  \u001b[A\n",
            "kNN:  59%|█████▉    | 23/39 [00:02<00:02,  7.53it/s, Accuracy=35]\u001b[A\n",
            "kNN:  59%|█████▉    | 23/39 [00:03<00:02,  7.53it/s, Accuracy=34.9]\u001b[A\n",
            "kNN:  62%|██████▏   | 24/39 [00:03<00:01,  7.72it/s, Accuracy=34.9]\u001b[A\n",
            "kNN:  62%|██████▏   | 24/39 [00:03<00:01,  7.72it/s, Accuracy=35]  \u001b[A\n",
            "kNN:  64%|██████▍   | 25/39 [00:03<00:01,  7.90it/s, Accuracy=35]\u001b[A\n",
            "kNN:  64%|██████▍   | 25/39 [00:03<00:01,  7.90it/s, Accuracy=35.1]\u001b[A\n",
            "kNN:  67%|██████▋   | 26/39 [00:03<00:01,  7.95it/s, Accuracy=35.1]\u001b[A\n",
            "kNN:  67%|██████▋   | 26/39 [00:03<00:01,  7.95it/s, Accuracy=35.2]\u001b[A\n",
            "kNN:  69%|██████▉   | 27/39 [00:03<00:01,  7.77it/s, Accuracy=35.2]\u001b[A\n",
            "kNN:  69%|██████▉   | 27/39 [00:03<00:01,  7.77it/s, Accuracy=35.1]\u001b[A\n",
            "kNN:  72%|███████▏  | 28/39 [00:03<00:01,  7.85it/s, Accuracy=35.1]\u001b[A\n",
            "kNN:  72%|███████▏  | 28/39 [00:03<00:01,  7.85it/s, Accuracy=34.9]\u001b[A\n",
            "kNN:  74%|███████▍  | 29/39 [00:03<00:01,  7.88it/s, Accuracy=34.9]\u001b[A\n",
            "kNN:  74%|███████▍  | 29/39 [00:03<00:01,  7.88it/s, Accuracy=34.9]\u001b[A\n",
            "kNN:  77%|███████▋  | 30/39 [00:03<00:01,  7.86it/s, Accuracy=34.9]\u001b[A\n",
            "kNN:  77%|███████▋  | 30/39 [00:03<00:01,  7.86it/s, Accuracy=34.9]\u001b[A\n",
            "kNN:  79%|███████▉  | 31/39 [00:03<00:01,  7.77it/s, Accuracy=34.9]\u001b[A\n",
            "kNN:  79%|███████▉  | 31/39 [00:04<00:01,  7.77it/s, Accuracy=35.1]\u001b[A\n",
            "kNN:  82%|████████▏ | 32/39 [00:04<00:00,  7.42it/s, Accuracy=35.1]\u001b[A\n",
            "kNN:  82%|████████▏ | 32/39 [00:04<00:00,  7.42it/s, Accuracy=35]  \u001b[A\n",
            "kNN:  85%|████████▍ | 33/39 [00:04<00:00,  7.47it/s, Accuracy=35]\u001b[A\n",
            "kNN:  85%|████████▍ | 33/39 [00:04<00:00,  7.47it/s, Accuracy=34.9]\u001b[A\n",
            "kNN:  87%|████████▋ | 34/39 [00:04<00:00,  7.51it/s, Accuracy=34.9]\u001b[A\n",
            "kNN:  87%|████████▋ | 34/39 [00:04<00:00,  7.51it/s, Accuracy=35.1]\u001b[A\n",
            "kNN:  90%|████████▉ | 35/39 [00:04<00:00,  7.53it/s, Accuracy=35.1]\u001b[A\n",
            "kNN:  90%|████████▉ | 35/39 [00:04<00:00,  7.53it/s, Accuracy=35.2]\u001b[A\n",
            "kNN:  92%|█████████▏| 36/39 [00:04<00:00,  7.41it/s, Accuracy=35.2]\u001b[A\n",
            "kNN:  92%|█████████▏| 36/39 [00:04<00:00,  7.41it/s, Accuracy=35.2]\u001b[A\n",
            "kNN:  95%|█████████▍| 37/39 [00:04<00:00,  7.24it/s, Accuracy=35.2]\u001b[A\n",
            "kNN:  95%|█████████▍| 37/39 [00:04<00:00,  7.24it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  97%|█████████▋| 38/39 [00:04<00:00,  7.40it/s, Accuracy=35.3]\u001b[A\n",
            "kNN:  97%|█████████▋| 38/39 [00:05<00:00,  7.40it/s, Accuracy=35.3]\u001b[A\n",
            "kNN: 100%|██████████| 39/39 [00:05<00:00,  7.69it/s, Accuracy=35.3]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 35.266426282051285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yDkCLmxSxWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf77a8f-b6a5-44d3-8370-cc909d42bad9"
      },
      "source": [
        "if args.eval is not False:\r\n",
        "  linear_eval(args, ckpt_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 1/30 [00:25<12:25, 25.71s/it]\u001b[A\n",
            "Evaluating:   7%|▋         | 2/30 [00:50<11:55, 25.54s/it]\u001b[A\n",
            "Evaluating:  10%|█         | 3/30 [01:16<11:27, 25.47s/it]\u001b[A\n",
            "Evaluating:  13%|█▎        | 4/30 [01:41<11:02, 25.49s/it]\u001b[A\n",
            "Evaluating:  17%|█▋        | 5/30 [02:07<10:36, 25.45s/it]\u001b[A\n",
            "Evaluating:  20%|██        | 6/30 [02:32<10:12, 25.53s/it]\u001b[A\n",
            "Evaluating:  23%|██▎       | 7/30 [02:58<09:45, 25.45s/it]\u001b[A\n",
            "Evaluating:  27%|██▋       | 8/30 [03:23<09:22, 25.57s/it]\u001b[A\n",
            "Evaluating:  30%|███       | 9/30 [03:49<08:57, 25.60s/it]\u001b[A\n",
            "Evaluating:  33%|███▎      | 10/30 [04:14<08:30, 25.54s/it]\u001b[A\n",
            "Evaluating:  37%|███▋      | 11/30 [04:40<08:04, 25.50s/it]\u001b[A\n",
            "Evaluating:  40%|████      | 12/30 [05:06<07:40, 25.56s/it]\u001b[A\n",
            "Evaluating:  43%|████▎     | 13/30 [05:32<07:21, 25.94s/it]\u001b[A\n",
            "Evaluating:  47%|████▋     | 14/30 [05:59<06:56, 26.04s/it]\u001b[A\n",
            "Evaluating:  50%|█████     | 15/30 [06:25<06:31, 26.12s/it]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 16/30 [06:51<06:04, 26.02s/it]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 17/30 [07:16<05:35, 25.78s/it]\u001b[A\n",
            "Evaluating:  60%|██████    | 18/30 [07:42<05:11, 25.99s/it]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 19/30 [08:09<04:47, 26.09s/it]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 20/30 [08:35<04:19, 25.98s/it]\u001b[A\n",
            "Evaluating:  70%|███████   | 21/30 [09:01<03:53, 25.99s/it]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 22/30 [09:27<03:28, 26.07s/it]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 23/30 [09:54<03:04, 26.29s/it]\u001b[A\n",
            "Evaluating:  80%|████████  | 24/30 [10:20<02:37, 26.20s/it]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 25/30 [10:46<02:11, 26.22s/it]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 26/30 [11:12<01:44, 26.23s/it]\u001b[A\n",
            "Evaluating:  90%|█████████ | 27/30 [11:38<01:18, 26.10s/it]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 28/30 [12:04<00:52, 26.09s/it]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 29/30 [12:30<00:26, 26.02s/it]\u001b[A\n",
            "Evaluating: 100%|██████████| 30/30 [12:56<00:00, 25.89s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 33.63\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}